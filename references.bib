
@inproceedings{newcombe_dynamicfusion_2015,
	address = {Boston, MA, USA},
	title = {{DynamicFusion}: {Reconstruction} and tracking of non-rigid scenes in real-time},
	isbn = {978-1-4673-6964-0},
	shorttitle = {{DynamicFusion}},
	url = {http://ieeexplore.ieee.org/document/7298631/},
	doi = {10.1109/CVPR.2015.7298631},
	abstract = {We present the ﬁrst dense SLAM system capable of reconstructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sensors. Our DynamicFusion approach reconstructs scene geometry whilst simultaneously estimating a dense volumetric 6D motion ﬁeld that warps the estimated geometry into a live frame. Like KinectFusion, our system produces increasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Newcombe, Richard A. and Fox, Dieter and Seitz, Steven M.},
	month = jun,
	year = {2015},
	pages = {343--352},
}

@inproceedings{sun_instance_2019,
	title = {Instance segmentation by using mask {R}-{CNN} based on feature fusion of {RGB} and depth images},
	volume = {11321},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11321/113210O/Instance-segmentation-by-using-mask-R-CNN-based-on-feature/10.1117/12.2542243.short},
	doi = {10.1117/12.2542243},
	abstract = {The instance segmentation for obstacle detection based on machine vision and deep learning is quite important for autonomous driving system. In this paper, a method using the Mask R-CNN based on feature fusion of RGB and depth images for instance segmentation is proposed. It extracts the features of depth image by designing a two-layer NiN network, and uses convolution to realize the feature fusion and dimension reduction of RGB image and depth image. The edge texture in depth image can improve the accuracy of boundary frame positioning. Experimental results on typical benchmark dataset demonstrates the effectiveness of the proposed method, which can improve the segmentation accuracy by 4\% and the recall rate by 2\%.},
	urldate = {2021-05-07},
	booktitle = {2019 {International} {Conference} on {Image} and {Video} {Processing}, and {Artificial} {Intelligence}},
	publisher = {International Society for Optics and Photonics},
	author = {Sun, Jinyu and Jin, Chengxiong and Ma, Shiwei},
	month = nov,
	year = {2019},
	pages = {113210O},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, ﬂexible, and general framework for object instance segmentation. Our approach efﬁciently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.},
	language = {en},
	urldate = {2021-05-07},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{danielczuk_segmenting_2019,
	title = {Segmenting {Unknown} {3D} {Objects} from {Real} {Depth} {Images} using {Mask} {R}-{CNN} {Trained} on {Synthetic} {Data}},
	url = {http://arxiv.org/abs/1809.05825},
	abstract = {The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment speciﬁc categories of objects in RGB images when massive handlabeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any handlabeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15\% in Average Precision and 20\% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and ﬁne-tuned on real images from the experimental setup. We deploy the model in an instance-speciﬁc grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.},
	language = {en},
	urldate = {2021-05-07},
	journal = {arXiv:1809.05825 [cs]},
	author = {Danielczuk, Michael and Matl, Matthew and Gupta, Saurabh and Li, Andrew and Lee, Andrew and Mahler, Jeffrey and Goldberg, Ken},
	month = mar,
	year = {2019},
	note = {arXiv: 1809.05825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{wang_depth-aware_2018,
	title = {Depth-aware {CNN} for {RGB}-{D} {Segmentation}},
	url = {http://arxiv.org/abs/1803.06791},
	abstract = {Convolutional neural networks (CNN) are limited by the lack of capability to handle geometric information due to the ﬁxed grid kernel structure. The availability of depth data enables progress in RGB-D semantic segmentation with CNNs. State-of-the-art methods either use depth as additional images or process spatial information in 3D volumes or point clouds. These methods suﬀer from high computation and memory cost. To address these issues, we present Depth-aware CNN by introducing two intuitive, ﬂexible and eﬀective operations: depth-aware convolution and depth-aware average pooling. By leveraging depth similarity between pixels in the process of information propagation, geometry is seamlessly incorporated into CNN. Without introducing any additional parameters, both operators can be easily integrated into existing CNNs. Extensive experiments and ablation studies on challenging RGB-D semantic segmentation benchmarks validate the eﬀectiveness and ﬂexibility of our approach.},
	language = {en},
	urldate = {2021-05-07},
	journal = {arXiv:1803.06791 [cs]},
	author = {Wang, Weiyue and Neumann, Ulrich},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.06791},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{qi_3d_2017,
	address = {Venice},
	title = {{3D} {Graph} {Neural} {Networks} for {RGBD} {Semantic} {Segmentation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237818/},
	doi = {10.1109/ICCV.2017.556},
	abstract = {RGBD semantic segmentation requires joint reasoning about 2D appearance and 3D geometric information. In this paper we propose a 3D graph neural network (3DGNN) that builds a k-nearest neighbor graph on top of 3D point cloud. Each node in the graph corresponds to a set of points and is associated with a hidden representation vector initialized with an appearance feature extracted by a unary CNN from 2D images. Relying on recurrent functions, every node dynamically updates its hidden representation based on the current status and incoming messages from its neighbors. This propagation model is unrolled for a certain number of time steps and the ﬁnal per-node representation is used for predicting the semantic class of each pixel. We use back-propagation through time to train the model. Extensive experiments on NYUD2 and SUN-RGBD datasets demonstrate the effectiveness of our approach.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Qi, Xiaojuan and Liao, Renjie and Jia, Jiaya and Fidler, Sanja and Urtasun, Raquel},
	month = oct,
	year = {2017},
	pages = {5209--5218},
}

@inproceedings{lin_cascaded_2017,
	address = {Venice},
	title = {Cascaded {Feature} {Network} for {Semantic} {Segmentation} of {RGB}-{D} {Images}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237409/},
	doi = {10.1109/ICCV.2017.147},
	abstract = {Fully convolutional network (FCN) has been successfully applied in semantic segmentation of scenes represented with RGB images. Images augmented with depth channel provide more understanding of the geometric information of the scene in the image. The question is how to best exploit this additional information to improve the segmentation performance.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Lin, Di and Chen, Guangyong and Cohen-Or, Daniel and Heng, Pheng-Ann and Huang, Hui},
	month = oct,
	year = {2017},
	pages = {1320--1328},
}

@inproceedings{gupta_perceptual_2013,
	address = {Portland, OR, USA},
	title = {Perceptual {Organization} and {Recognition} of {Indoor} {Scenes} from {RGB}-{D} {Images}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6618923/},
	doi = {10.1109/CVPR.2013.79},
	abstract = {We address the problems of contour detection, bottomup grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gP b − ucm approach of [2] by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classiﬁes superpixels into the 40 dominant object categories in NYUD2. We use both generic and class-speciﬁc features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classiﬁcation, and how this contextual information in turn improves object recognition. In all of these tasks, we report signiﬁcant improvements over the state-of-the-art.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Gupta, Saurabh and Arbelaez, Pablo and Malik, Jitendra},
	month = jun,
	year = {2013},
	pages = {564--571},
}

@inproceedings{lee_rdfnet_2017,
	address = {Venice},
	title = {{RDFNet}: {RGB}-{D} {Multi}-level {Residual} {Feature} {Fusion} for {Indoor} {Semantic} {Segmentation}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{RDFNet}},
	url = {http://ieeexplore.ieee.org/document/8237795/},
	doi = {10.1109/ICCV.2017.533},
	abstract = {In multi-class indoor semantic segmentation using RGBD data, it has been shown that incorporating depth feature into RGB feature is helpful to improve segmentation accuracy. However, previous studies have not fully exploited the potentials of multi-modal feature fusion, e.g., simply concatenating RGB and depth features or averaging RGB and depth score maps. To learn the optimal fusion of multimodal features, this paper presents a novel network that extends the core idea of residual learning to RGB-D semantic segmentation. Our network effectively captures multilevel RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature reﬁnement blocks. Feature fusion blocks learn residual RGB and depth features and their combinations to fully exploit the complementary characteristics of RGB and depth data. Feature reﬁnement blocks learn the combination of fused features from multiple levels to enable high-resolution prediction. Our network can efﬁciently train discriminative multi-level features from each modality end-to-end by taking full advantage of skip-connections. Our comprehensive experiments demonstrate that the proposed architecture achieves the state-of-the-art accuracy on two challenging RGB-D indoor datasets, NYUDv2 and SUN RGB-D.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Lee, Seungyong and Park, Seong-Jin and Hong, Ki-Sang},
	month = oct,
	year = {2017},
	pages = {4990--4999},
}

@inproceedings{wang_deep_2019,
	title = {Deep {Closest} {Point}: {Learning} {Representations} for {Point} {Cloud} {Registration}},
	shorttitle = {Deep {Closest} {Point}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.html},
	urldate = {2021-05-06},
	author = {Wang, Yue and Solomon, Justin M.},
	year = {2019},
	pages = {3523--3532},
}

@article{lee_2-d_2020,
	title = {2-{D} {Deep} {Learning} {Model} on 3-{D} {Image} {Segmentation}},
	doi = {10.1109/ICMA49215.2020.9233871},
	abstract = {For image segmentation of 3D objects, researchers recently focus on 3D image deep learning methods. These studies aim at developing robust and accurate deep learning models. However, 3D deep learning methods using point-cloud data are time-consuming. In this paper, we present a novel 3D image segmentation method based on a 2D deep learning model to achieve efficient segmentation performance. While using a single camera angle to collect object 3D information, we merely use the depth map and adopt a 2D deep-learning model to segment objects on the scene. We validated the proposed method by comparison with Point-Net. In our experiment, we provided an extensive comparison between Point-Net and our 2D deep-learning model. The result shows that our model had a similar accuracy but is much faster than Point-Net.},
	journal = {2020 IEEE International Conference on Mechatronics and Automation (ICMA)},
	author = {Lee, Chien-Chia and Lin, Hsien-I.},
	year = {2020},
}

@inproceedings{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html},
	urldate = {2021-05-04},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	year = {2017},
	pages = {652--660},
}

@inproceedings{tchapmi_segcloud_2017,
	title = {{SEGCloud}: {Semantic} {Segmentation} of {3D} {Point} {Clouds}},
	shorttitle = {{SEGCloud}},
	doi = {10.1109/3DV.2017.00067},
	abstract = {3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-the-art on all datasets.},
	booktitle = {2017 {International} {Conference} on {3D} {Vision} ({3DV})},
	author = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, JunYoung and Savarese, Silvio},
	month = oct,
	year = {2017},
	note = {ISSN: 2475-7888},
	keywords = {3D-Conditional-Random-Fields, 3D-Convolutional-Neural-Networks, 3D-Point-Clouds, 3D-Semantic-Segmentation, Computer architecture, Convolutional neural networks, Interpolation, Labeling, RGB-D, Semantics, Three-dimensional displays},
	pages = {537--547},
}

@article{qi_pointnet_2017-1,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2021-05-04},
	journal = {arXiv:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02413},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{griffiths_review_2019,
	title = {A {Review} on {Deep} {Learning} {Techniques} for {3D} {Sensed} {Data} {Classification}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/11/12/1499},
	doi = {10.3390/rs11121499},
	abstract = {Over the past decade deep learning has driven progress in 2D image understanding. Despite these advancements, techniques for automatic 3D sensed data understanding, such as point clouds, is comparatively immature. However, with a range of important applications from indoor robotics navigation to national scale remote sensing there is a high demand for algorithms that can learn to automatically understand and classify 3D sensed data. In this paper we review the current state-of-the-art deep learning architectures for processing unstructured Euclidean data. We begin by addressing the background concepts and traditional methodologies. We review the current main approaches, including RGB-D, multi-view, volumetric and fully end-to-end architecture designs. Datasets for each category are documented and explained. Finally, we give a detailed discussion about the future of deep learning for 3D sensed data, using literature to justify the areas where future research would be most valuable.},
	language = {en},
	number = {12},
	urldate = {2021-05-04},
	journal = {Remote Sensing},
	author = {Griffiths, David and Boehm, Jan},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {classification, deep learning, machine learning, point cloud, segmentation, semantics},
	pages = {1499},
}

@article{he_pointshufflenet_2021,
	title = {{PointShuffleNet}: {Learning} {Non}-{Euclidean} {Features} with {Homotopy} {Equivalence} and {Mutual} {Information}},
	shorttitle = {{PointShuffleNet}},
	url = {http://arxiv.org/abs/2104.02611},
	abstract = {Point cloud analysis is still a challenging task due to the disorder and sparsity of samplings of their geometric structures from 3D sensors. In this paper, we introduce the homotopy equivalence relation (HER) to make the neural networks learn the data distribution from a high-dimension manifold. A shufﬂe operation is adopted to construct HER for its randomness and zero-parameter. In addition, inspired by prior works, we propose a local mutual information regularizer (LMIR) to cut off the trivial path that leads to a classiﬁcation error from HER. LMIR utilizes mutual information to measure the distance between the original feature and HER transformed feature and learns common features in a contrastive learning scheme. Thus, we combine HER and LMIR to give our model the ability to learn non-Euclidean features from a high-dimension manifold. This is named the non-Euclidean feature learner. Furthermore, we propose a new heuristics and efﬁciency point sampling algorithm named ClusterFPS to obtain approximate uniform sampling but at faster speed. ClusterFPS uses a cluster algorithm to divide a point cloud into several clusters and deploy the farthest point sampling algorithm on each cluster in parallel. By combining the above methods, we propose a novel point cloud analysis neural network called PointShufﬂeNet (PSN), which shows great promise in point cloud classiﬁcation and segmentation. Extensive experiments show that our PSN achieves state-ofthe-art results on ModelNet40, ShapeNet and S3DIS with high efﬁciency. Theoretically, we provide mathematical analysis toward understanding of what the data distribution HER has developed and why LMIR can drop the trivial path by maximizing mutual information implicitly.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2104.02611 [cs]},
	author = {He, Linchao and Luo, Mengting and Zhang, Dejun and Yang, Xiao and Chen, Hu and Zhang, Yi},
	month = mar,
	year = {2021},
	note = {arXiv: 2104.02611},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_gapointnet_2021,
	title = {{GAPointNet}: {Graph} attention based point neural network for exploiting local feature of point cloud},
	volume = {438},
	issn = {0925-2312},
	shorttitle = {{GAPointNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221001776},
	doi = {10.1016/j.neucom.2021.01.095},
	abstract = {Exploiting fine-grained semantic features on point cloud data is still challenging because of its irregular and sparse structure in a non-Euclidean space. In order to represent the local feature for each central point that is helpful towards better contextual learning, a max pooling operation is often used to highlight the most important feature in the local region. However, all other geometric local correlations between each central point and corresponding neighbourhood are ignored during the max pooling operation. To this end, the attention mechanism is promising in capturing node representation on graph-based data by attending over all the neighbouring nodes. In this paper, we propose a novel neural network for point cloud analysis, GAPointNet, which is able to learn local geometric representations by embedding graph attention mechanism within stacked Multi-Layer-Perceptron (MLP) layers. Specifically, we highlight different attention weights on the neighbourhood of each center point to efficiently exploit local features. We also combine attention features with local signature features generated by our attention pooling to fully extract local geometric structures and enhance the network robustness. The proposed GAPointNet architecture is tested on various benchmark datasets (i.e. ModelNet40, ShapeNet part, S3DIS, KITTI) and achieves state-of-the-art performance in both the shape classification and segmentation tasks.},
	language = {en},
	urldate = {2021-05-03},
	journal = {Neurocomputing},
	author = {Chen, Can and Fragonara, Luca Zanotti and Tsourdos, Antonios},
	month = may,
	year = {2021},
	keywords = {Attention pooling, Graph attention, Multiple heads mechanism, Point cloud, Semantic segmentation, Shape classification},
	pages = {122--132},
}

@inproceedings{zhan_self-supervised_2020,
	address = {Seattle, WA, USA},
	title = {Self-{Supervised} {Scene} {De}-{Occlusion}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156608/},
	doi = {10.1109/CVPR42600.2020.00384},
	language = {en},
	urldate = {2021-05-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhan, Xiaohang and Pan, Xingang and Dai, Bo and Liu, Ziwei and Lin, Dahua and Loy, Chen Change},
	month = jun,
	year = {2020},
	pages = {3783--3791},
}

@inproceedings{zhang_object-occluded_2020,
	address = {Seattle, WA, USA},
	title = {Object-{Occluded} {Human} {Shape} and {Pose} {Estimation} {From} a {Single} {Color} {Image}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157481/},
	doi = {10.1109/CVPR42600.2020.00740},
	abstract = {Occlusions between human and objects, especially for the activities of human-object interactions, are very common in practical applications. However, most of the existing approaches for 3D human shape and pose estimation require human bodies are well captured without occlusions or with minor self-occlusions. In this paper, we focus on the problem of directly estimating the object-occluded human shape and pose from single color images. Our key idea is to utilize a partial UV map to represent an objectoccluded human body, and the full 3D human shape estimation is ultimately converted as an image inpainting problem. We propose a novel two-branch network architecture to train an end-to-end regressor via the latent feature supervision, which also includes a novel saliency map subnet to extract the human information from object-occluded color images. To supervise the network training, we further build a novel dataset named as 3DOH50K. Several experiments are conducted to reveal the effectiveness of the proposed method. Experimental results demonstrate that the proposed method achieves the state-of-the-art comparing with previous methods. The dataset, codes are publicly available at https://www.yangangwang.com.},
	language = {en},
	urldate = {2021-05-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Tianshu and Huang, Buzhen and Wang, Yangang},
	month = jun,
	year = {2020},
	pages = {7374--7383},
}

@inproceedings{schulman_tracking_2013,
	title = {Tracking deformable objects with point clouds},
	doi = {10.1109/ICRA.2013.6630714},
	abstract = {We introduce an algorithm for tracking deformable objects from a sequence of point clouds. The proposed tracking algorithm is based on a probabilistic generative model that incorporates observations of the point cloud and the physical properties of the tracked object and its environment. We propose a modified expectation maximization algorithm to perform maximum a posteriori estimation to update the state estimate at each time step. Our modification makes it practical to perform the inference through calls to a physics simulation engine. This is significant because (i) it allows for the use of highly optimized physics simulation engines for the core computations of our tracking algorithm, and (ii) it makes it possible to naturally, and efficiently, account for physical constraints imposed by collisions, grasping actions, and material properties in the observation updates. Even in the presence of the relatively large occlusions that occur during manipulation tasks, our algorithm is able to robustly track a variety of types of deformable objects, including ones that are one-dimensional, such as ropes; two-dimensional, such as cloth; and three-dimensional, such as sponges. Our implementation can track these objects in real time.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Schulman, John and Lee, Alex and Ho, Jonathan and Abbeel, Pieter},
	month = may,
	year = {2013},
	note = {ISSN: 1050-4729},
	keywords = {Computational modeling, Inference algorithms, Mathematical model, Noise, Physics, Probabilistic logic, Solid modeling},
	pages = {1130--1137},
}

@article{maddalena_background_2018,
	title = {Background {Subtraction} for {Moving} {Object} {Detection} in {RGBD} {Data}: {A} {Survey}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Background {Subtraction} for {Moving} {Object} {Detection} in {RGBD} {Data}},
	url = {https://www.mdpi.com/2313-433X/4/5/71},
	doi = {10.3390/jimaging4050071},
	abstract = {The paper provides a specific perspective view on background subtraction for moving object detection, as a building block for many computer vision applications, being the first relevant step for subsequent recognition, classification, and activity analysis tasks. Since color information is not sufficient for dealing with problems like light switches or local gradual changes of illumination, shadows cast by the foreground objects, and color camouflage, new information needs to be caught to deal with these issues. Depth synchronized information acquired by low-cost RGBD sensors is considered in this paper to give evidence about which issues can be solved, but also to highlight new challenges and design opportunities in several applications and research areas.},
	language = {en},
	number = {5},
	urldate = {2021-04-29},
	journal = {Journal of Imaging},
	author = {Maddalena, Lucia and Petrosino, Alfredo},
	month = may,
	year = {2018},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {RGBD, background subtraction, color and depth data},
	pages = {71},
}

@article{majidi_soft_2013,
	title = {Soft {Robotics}: {A} {Perspective}—{Current} {Trends} and {Prospects} for the {Future}},
	volume = {1},
	issn = {2169-5172},
	shorttitle = {Soft {Robotics}},
	url = {https://www.liebertpub.com/doi/10.1089/soro.2013.0001},
	doi = {10.1089/soro.2013.0001},
	abstract = {Soft robots are primarily composed of easily deformable matter such as fluids, gels, and elastomers that match the elastic and rheological properties of biological tissue and organs. Like an octopus squeezing through a narrow opening or a caterpillar rolling through uneven terrain, a soft robot must adapt its shape and locomotion strategy for a broad range of tasks, obstacles, and environmental conditions. This emerging class of elastically soft, versatile, and biologically inspired machines represents an exciting and highly interdisciplinary paradigm in engineering that could revolutionize the role of robotics in healthcare, field exploration, and cooperative human assistance.},
	number = {1},
	urldate = {2021-04-23},
	journal = {Soft Robotics},
	author = {Majidi, Carmel},
	month = jul,
	year = {2013},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	pages = {5--11},
}

@article{majidi_soft_nodate,
	title = {Soft {Robotics}: {A} {Perspective}—{Current} {Trends} and {Prospects} for the {Future}},
	abstract = {Soft robots are primarily composed of easily deformable matter such as ﬂuids, gels, and elastomers that match the elastic and rheological properties of biological tissue and organs. Like an octopus squeezing through a narrow opening or a caterpillar rolling through uneven terrain, a soft robot must adapt its shape and locomotion strategy for a broad range of tasks, obstacles, and environmental conditions. This emerging class of elastically soft, versatile, and biologically inspired machines represents an exciting and highly interdisciplinary paradigm in engineering that could revolutionize the role of robotics in healthcare, ﬁeld exploration, and cooperative human assistance.},
	language = {en},
	journal = {SOFT ROBOTICS},
	author = {Majidi, Carmel},
	pages = {7},
}

@article{hughes_soft_2016,
	title = {Soft {Manipulators} and {Grippers}: {A} {Review}},
	volume = {3},
	issn = {2296-9144},
	shorttitle = {Soft {Manipulators} and {Grippers}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2016.00069/full},
	doi = {10.3389/frobt.2016.00069},
	abstract = {Soft robotics is a growing area of research which utilises the compliance and adaptability of soft structures to develop highly adaptive robotics for soft interactions. One area in which soft robotics has the ability to make significant impact is in the development of soft grippers and manipulators. With an increased requirement for automation, robotics systems are required to perform task in unstructured and not well defined environments; conditions which conventional rigid robotics are not best suited. This requires a paradigm shift in the methods and materials used to develop robots such that they can adapt to and work safely in human environments. One solution to this is soft robotics, which enables soft interactions with the surroundings whilst maintaining the ability to apply significant force. This review paper assess the current materials and methods, actuation methods and sensors which are used in the development of soft manipulators. The achievements and shortcomings of recent technology in these key areas are evaluated, and this paper concludes with a discussion on the potential impacts of soft manipulators on industry and society.},
	language = {English},
	urldate = {2021-04-23},
	journal = {Frontiers in Robotics and AI},
	author = {Hughes, Josie and Culha, Utku and Giardina, Fabio and Guenther, Fabian and Rosendo, Andre and Iida, Fumiya},
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {bioinspiration, end effect, manipulator robot, soft robotics, stiffness control},
}

@inproceedings{walsman_dynamic_2017,
	address = {Qingdao},
	title = {Dynamic {High} {Resolution} {Deformable} {Articulated} {Tracking}},
	isbn = {978-1-5386-2610-8},
	url = {https://ieeexplore.ieee.org/document/8374556/},
	doi = {10.1109/3DV.2017.00015},
	abstract = {The last several years have seen signiﬁcant progress in using depth cameras for tracking articulated objects such as human bodies, hands, and robotic manipulators. Most approaches focus on tracking skeletal parameters of a ﬁxed shape model, which makes them insufﬁcient for applications that require accurate estimates of deformable object surfaces. To overcome this limitation, we present a 3D modelbased tracking system for articulated deformable objects. Our system is able to track human body pose and high resolution surface contours in real time using a commodity depth sensor and GPU hardware. We implement this as a joint optimization over a skeleton to account for changes in pose, and over the vertices of a high resolution mesh to track the subject’s shape. Through experimental results we show that we are able to capture dynamic sub-centimeter surface detail such as folds and wrinkles in clothing. We also show that this shape estimation aids kinematic pose estimation by providing a more accurate target to match against the point cloud. The end result is highly accurate spatiotemporal and semantic information which is well suited for physical human robot interaction as well as virtual and augmented reality systems.},
	language = {en},
	urldate = {2021-04-21},
	booktitle = {2017 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Walsman, Aaron and Wan, Weilin and Schmidt, Tanner and Fox, Dieter},
	month = oct,
	year = {2017},
	pages = {38--47},
}

@article{gao_fast_2017,
	title = {Fast and robust image segmentation with active contours and {Student}'s-t mixture model},
	volume = {63},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316302734},
	doi = {10.1016/j.patcog.2016.09.014},
	abstract = {In this paper, a novel active contours method, which combines with the Student's-t mixture model via Expectaton-Maximizaton (EM) algorithm, is proposed to segment complicated two-phase images. Firstly, we rewrite the cost function and derive a novel updating of level set function based on probabilistic principles. Secondly, we put forward two novel geometric priors from the level-set-based curve evolution; and both of them have advantages, the suitable one is selected by personalized need to obtain level set function in EM framework with the aim of reducing the computational cost. Therefore, the level set function is derived from latent variables and served as a feedback to the estimation of the latent variables in next iteration. Finally, in order to enhance the robustness to the outliers, Student's-t mixture model with heavy tail has been applied in our algorithm. Experimental results obtained by employing the proposed method on many synthetic, medical and real-world images to demonstrate its robustness, accuracy and effectiveness.},
	language = {en},
	urldate = {2021-04-21},
	journal = {Pattern Recognition},
	author = {Gao, Guowei and Wen, Chenglin and Wang, Huibin},
	month = mar,
	year = {2017},
	keywords = {Active contours, EM algorithm, Level set, Segmentation, Student's-t mixture model},
	pages = {71--86},
}

@incollection{leibe_volumedeform_2016,
	address = {Cham},
	title = {{VolumeDeform}: {Real}-{Time} {Volumetric} {Non}-rigid {Reconstruction}},
	volume = {9912},
	isbn = {978-3-319-46483-1 978-3-319-46484-8},
	shorttitle = {{VolumeDeform}},
	url = {http://link.springer.com/10.1007/978-3-319-46484-8_22},
	abstract = {We present a novel approach for the reconstruction of dynamic geometric shapes using a single hand-held consumer-grade RGB-D sensor at real-time rates. Our method builds up the scene model from scratch during the scanning process, thus it does not require a predeﬁned shape template to start with. Geometry and motion are parameterized in a uniﬁed manner by a volumetric representation that encodes a distance ﬁeld of the surface geometry as well as the non-rigid space deformation. Motion tracking is based on a set of extracted sparse color features in combination with a dense depth constraint. This enables accurate tracking and drastically reduces drift inherent to standard modelto-depth alignment. We cast ﬁnding the optimal deformation of space as a non-linear regularized variational optimization problem by enforcing local smoothness and proximity to the input constraints. The problem is tackled in real-time at the camera’s capture rate using a data-parallel ﬂip-ﬂop optimization strategy. Our results demonstrate robust tracking even for fast motion and scenes that lack geometric features.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Innmann, Matthias and Zollhöfer, Michael and Nießner, Matthias and Theobalt, Christian and Stamminger, Marc},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46484-8_22},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {362--379},
}

@inproceedings{huang_control_2018,
	title = {Control of a piecewise constant curvature continuum manipulator via policy search method},
	doi = {10.1109/ROBIO.2018.8665152},
	abstract = {Accurate dynamic models are necessary for fast and precise motion control of continuum manipulators, but can hardly be obtained. In this paper, we propose a method for precise position control of piecewise constant curvature (PCC) continuum manipulators using the guided policy search method. The policy network is trained to control the manipulator to reach target position precisely. Firstly, a linear time-varying Gaussian model fitting to sample trajectories is used to approximate the dynamics of the PCC continuum manipulator. Then, the Gaussian controller that induces the trajectory distribution is optimized by iterative LQR based on the fitted model. Finally, the policy is updated by minimizing the KL-divergence between the policy and trajectory distribution. We have simulated the PCC continuum manipulator with two-segment model and three-segment model, respectively. The results show that the linear time-varying Gaussian model can describe the dynamics of the PCC continuum manipulator and the proposed method can achieve desired performance of position control within a couple of iterations.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Huang, Sheng and Zhang, Qiyuan and Liu, Zhaoyang and Wang, Xueqian and Liang, Bin},
	month = dec,
	year = {2018},
	keywords = {Heuristic algorithms, Manipulator dynamics, Mathematical model, Piecewise constant curvature, Power cables, Trajectory, continuum robot, guided policy search, reinforcement learning},
	pages = {1777--1782},
}

@inproceedings{runge_framework_2017,
	title = {A framework for the kinematic modeling of soft material robots combining finite element analysis and piecewise constant curvature kinematics},
	doi = {10.1109/ICCAR.2017.7942652},
	abstract = {Recent advances in the rapidly growing field of soft robotics have shown that robots made from soft materials surpass traditional rigid-link and continuum robots in performance in terms of adaptability and flexibility. Owing to their structure-inherent compliance, soft robots are also credited with being able to bridge the gap between humans and autonomous machines. However, for soft robots to become mass-producible in future days, rapid design and modeling tools are indispensable. Currently, the need for fast and versatile development tools is contradicted by a design flow that is mainly driven by experiments. To counter the perceived gap between experimentally and simulation driven research in soft robotics, we have developed a finite element based kinematic modeling framework for soft continuum robots that is applicable to a range of different designs. The corresponding design and modeling tool effectively combines the capabilities of both MATLAB and Abaqus via a Python interface. In this paper, we provide detailed description of the developed methodology and demonstrate the feasibility of the approach on a soft pneumatic actuator.},
	booktitle = {2017 3rd {International} {Conference} on {Control}, {Automation} and {Robotics} ({ICCAR})},
	author = {Runge, G. and Wiese, M. and Günther, L. and Raatz, A.},
	month = apr,
	year = {2017},
	keywords = {Actuators, Analytical models, Deformable models, Kinematics, MATLAB, Mathematical model, Robots, finite element analysis, kinematic modeling, soft pneumatic actuators, soft robotics},
	pages = {7--14},
}

@inproceedings{curless_volumetric_1996,
	address = {Not Known},
	title = {A volumetric method for building complex models from range images},
	isbn = {978-0-89791-746-9},
	url = {http://portal.acm.org/citation.cfm?doid=237170.237269},
	doi = {10.1145/237170.237269},
	abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to ﬁll gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties.},
	language = {en},
	urldate = {2021-04-14},
	booktitle = {Proceedings of the 23rd annual conference on {Computer} graphics and interactive techniques  - {SIGGRAPH} '96},
	publisher = {ACM Press},
	author = {Curless, Brian and Levoy, Marc},
	year = {1996},
	pages = {303--312},
}

@article{tobin_domain_2017,
	title = {Domain {Randomization} for {Transferring} {Deep} {Neural} {Networks} from {Simulation} to the {Real} {World}},
	url = {http://arxiv.org/abs/1703.06907},
	abstract = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We ﬁnd that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the ﬁrst successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	language = {en},
	urldate = {2021-04-14},
	journal = {arXiv:1703.06907 [cs]},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.06907},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{finman_toward_2013,
	address = {Barcelona, Catalonia, Spain},
	title = {Toward lifelong object segmentation from change detection in dense {RGB}-{D} maps},
	isbn = {978-1-4799-0263-7},
	url = {http://ieeexplore.ieee.org/document/6698839/},
	doi = {10.1109/ECMR.2013.6698839},
	abstract = {In this paper, we present a system for automatically learning segmentations of objects given changes in dense RGB-D maps over the lifetime of a robot. Using recent advances in RGBD mapping to construct multiple dense maps, we detect changes between mapped regions from multiple traverses by performing a 3-D difference of the scenes. Our method takes advantage of the free space seen in each map to account for variability in how the maps were created. The resulting changes from the 3D difference are our discovered objects, which are then used to train multiple segmentation algorithms in the original map. The ﬁnal objects can then be matched in other maps given their corresponding features and learned segmentation method. If the same object is discovered multiple times in different contexts, the features and segmentation method are reﬁned, incorporating all instances to better learn objects over time. We verify our approach with multiple objects in numerous and varying maps.},
	language = {en},
	urldate = {2021-04-14},
	booktitle = {2013 {European} {Conference} on {Mobile} {Robots}},
	publisher = {IEEE},
	author = {Finman, Ross and Whelan, Thomas and Kaess, Michael and Leonard, John J.},
	month = sep,
	year = {2013},
	pages = {178--185},
}

@inproceedings{li_benchmark_2013,
	title = {A benchmark for semantic image segmentation},
	doi = {10.1109/ICME.2013.6607512},
	abstract = {Though quite a few image segmentation benchmark datasets have been constructed, there is no suitable benchmark for semantic image segmentation. In this paper, we construct a benchmark for such a purpose, where the ground-truths are generated by leveraging the existing fine granular ground-truths in Berkeley Segmentation Dataset (BSD) as well as using an interactive segmentation tool for new images. We also propose a percept-tree-based region merging strategy for dynamically adapting the ground-truth for evaluating test segmentation. Moreover, we propose a new evaluation metric that is easy to understand and compute, and does not require boundary matching. Experimental results show that, compared with BSD, the generated ground-truth dataset is more suitable for evaluating semantic image segmentation, and the conducted user study demonstrates that the proposed evaluation metric matches user ranking very well.},
	author = {Li, Hui and Cai, Jianfei and Nguyen, Anh and Zheng, Jianmin},
	month = jul,
	year = {2013},
	pages = {1--6},
}

@article{fooladgar_survey_2020,
	title = {A survey on indoor {RGB}-{D} semantic segmentation: from hand-crafted features to deep convolutional neural networks},
	volume = {79},
	issn = {1573-7721},
	shorttitle = {A survey on indoor {RGB}-{D} semantic segmentation},
	url = {https://doi.org/10.1007/s11042-019-7684-3},
	doi = {10.1007/s11042-019-7684-3},
	abstract = {Semantic segmentation is one of the most important tasks in the field of computer vision. It is the main step towards scene understanding. With the advent of RGB-Depth sensors, such as Microsoft Kinect, nowadays RGB-Depth images are easily available. This has changed the landscape of some tasks such as semantic segmentation. As the depth images are independent of illumination, the combination of depth and RGB images can improve the quality of semantic labeling. The related research has been divided into two main categories, based on the usage of hand-crafted features and deep learning. Although the state-of-the-art results are mainly achieved by deep learning methods, traditional methods have also been at the center of attention for some years and lots of valuable work have been done in that category. As the field of semantic segmentation is very broad, in this survey, a comprehensive analysis has been carried out on RGB-Depth semantic segmentation methods, their challenges and contributions, available RGB-Depth datasets, metrics of evaluation, state-of-the-art results, and promising directions of the field.},
	language = {en},
	number = {7},
	urldate = {2021-04-06},
	journal = {Multimedia Tools and Applications},
	author = {Fooladgar, Fahimeh and Kasaei, Shohreh},
	month = feb,
	year = {2020},
	pages = {4499--4524},
}

@book{salzmann_deformable_2010,
	title = {Deformable {Surface} {3D} {Reconstruction} from {Monocular} {Images}},
	volume = {2},
	abstract = {Being able to recover the shape of 3D deformable surfaces from a single video stream would make it possible to field reconstruction systems that run on widely available hardware without requiring specialized devices. However, because many different 3D shapes can have virtually the same projection, such monocular shape recovery is inherently ambiguous. In this survey, we will review the two main classes of techniques that have proved most effective so far: The template-based methods that rely on establishing correspondences with a reference image in which the shape is already known, and non-rigid structure-from-motion techniques that exploit points tracked across the sequences to reconstruct a completely unknown shape. In both cases, we will formalize the approach, discuss its inherent ambiguities, and present the practical solutions that have been proposed to resolve them. To conclude, we will suggest directions for future research.},
	author = {Salzmann, Mathieu and Fua, Pascal},
	month = sep,
	year = {2010},
	doi = {10.2200/S00319ED1V01Y201012COV003},
	note = {Journal Abbreviation: Synthesis Lectures on Computer Vision
Publication Title: Synthesis Lectures on Computer Vision},
}

@article{nash_polygen_2020,
	title = {{PolyGen}: {An} {Autoregressive} {Generative} {Model} of {3D} {Meshes}},
	shorttitle = {{PolyGen}},
	url = {http://arxiv.org/abs/2002.10880},
	abstract = {Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.},
	urldate = {2021-04-01},
	journal = {arXiv:2002.10880 [cs, stat]},
	author = {Nash, Charlie and Ganin, Yaroslav and Eslami, S. M. Ali and Battaglia, Peter W.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.10880},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hanocka_meshcnn_2019,
	title = {{MeshCNN}: {A} {Network} with an {Edge}},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{MeshCNN}},
	url = {http://arxiv.org/abs/1809.05910},
	doi = {10.1145/3306346.3322959},
	abstract = {Polygonal meshes provide an efficient representation for 3D shapes. They explicitly capture both shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of our task-driven pooling on various learning tasks applied to 3D meshes.},
	number = {4},
	urldate = {2021-04-01},
	journal = {ACM Transactions on Graphics},
	author = {Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},
	month = jul,
	year = {2019},
	note = {arXiv: 1809.05910},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--12},
}

@article{bronstein_geometric_2017,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	volume = {34},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Geometric deep learning},
	url = {http://arxiv.org/abs/1611.08097},
	doi = {10.1109/MSP.2017.2693418},
	abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	number = {4},
	urldate = {2021-04-01},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = jul,
	year = {2017},
	note = {arXiv: 1611.08097},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {18--42},
}

@article{pfaff_learning_2021,
	title = {Learning {Mesh}-{Based} {Simulation} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2010.03409},
	abstract = {Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.},
	urldate = {2021-04-01},
	journal = {arXiv:2010.03409 [cs]},
	author = {Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W.},
	month = jan,
	year = {2021},
	note = {arXiv: 2010.03409},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning},
}

@article{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.},
	urldate = {2021-04-01},
	journal = {arXiv:2002.09405 [physics, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = sep,
	year = {2020},
	note = {arXiv: 2002.09405},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
}

@inproceedings{petit_real-time_2015,
	address = {Hamburg, Germany},
	title = {Real-time tracking of {3D} elastic objects with an {RGB}-{D} sensor},
	isbn = {978-1-4799-9994-1},
	url = {http://ieeexplore.ieee.org/document/7353928/},
	doi = {10.1109/IROS.2015.7353928},
	abstract = {This paper presents a method to track in real-time a 3D textureless object which undergoes large deformations such as elastic ones, and rigid motions, using the point cloud data provided by an RGB-D sensor. This solution is expected to be useful for enhanced manipulation of humanoid robotic systems. Our framework relies on a prior visual segmentation of the object in the image. The segmented point cloud is registered ﬁrst in a rigid manner and then by non-rigidly ﬁtting the mesh, based on the Finite Element Method to model elasticity, and on geometrical point-to-point correspondences to compute external forces exerted on the mesh. The real-time performance of the system is demonstrated on synthetic and real data involving challenging deformations and motions.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Petit, Antoine and Lippiello, Vincenzo and Siciliano, Bruno},
	month = sep,
	year = {2015},
	pages = {3914--3921},
}

@inproceedings{bozic_deepdeform_2020,
	address = {Seattle, WA, USA},
	title = {{DeepDeform}: {Learning} {Non}-{Rigid} {RGB}-{D} {Reconstruction} {With} {Semi}-{Supervised} {Data}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{DeepDeform}},
	url = {https://ieeexplore.ieee.org/document/9156355/},
	doi = {10.1109/CVPR42600.2020.00703},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bozic, Aljaz and Zollhofer, Michael and Theobalt, Christian and NieBner, Matthias},
	month = jun,
	year = {2020},
	pages = {7000--7010},
}

@article{li_online_2020,
	title = {Online {Adaptation} for {Consistent} {Mesh} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2012.03196},
	abstract = {This paper presents an algorithm to reconstruct temporally consistent 3D meshes of deformable object instances from videos in the wild. Without requiring annotations of 3D mesh, 2D keypoints, or camera pose for each video frame, we pose video-based reconstruction as a self-supervised online adaptation problem applied to any incoming test video. We first learn a category-specific 3D reconstruction model from a collection of single-view images of the same category that jointly predicts the shape, texture, and camera pose of an image. Then, at inference time, we adapt the model to a test video over time using self-supervised regularization terms that exploit temporal consistency of an object instance to enforce that all reconstructed meshes share a common texture map, a base shape, as well as parts. We demonstrate that our algorithm recovers temporally consistent and reliable 3D structures from videos of non-rigid objects including those of animals captured in the wild -- an extremely challenging task rarely addressed before.},
	urldate = {2021-03-25},
	journal = {arXiv:2012.03196 [cs]},
	author = {Li, Xueting and Liu, Sifei and De Mello, Shalini and Kim, Kihwan and Wang, Xiaolong and Yang, Ming-Hsuan and Kautz, Jan},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.03196},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ahmed_survey_2019,
	title = {A survey on {Deep} {Learning} {Advances} on {Different} {3D} {Data} {Representations}},
	url = {http://arxiv.org/abs/1808.01462},
	abstract = {3D data is a valuable asset the computer vision filed as it provides rich information about the full geometry of sensed objects and scenes. Recently, with the availability of both large 3D datasets and computational power, it is today possible to consider applying deep learning to learn specific tasks on 3D data such as segmentation, recognition and correspondence. Depending on the considered 3D data representation, different challenges may be foreseen in using existent deep learning architectures. In this work, we provide a comprehensive overview about various 3D data representations highlighting the difference between Euclidean and non-Euclidean ones. We also discuss how Deep Learning methods are applied on each representation, analyzing the challenges to overcome.},
	language = {en},
	urldate = {2021-03-25},
	journal = {arXiv:1808.01462 [cs]},
	author = {Ahmed, Eman and Saint, Alexandre and Shabayek, Abd El Rahman and Cherenkova, Kseniya and Das, Rig and Gusev, Gleb and Aouada, Djamila and Ottersten, Bjorn},
	month = apr,
	year = {2019},
	note = {arXiv: 1808.01462},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{pan_deep_2019,
	address = {Seoul, Korea (South)},
	title = {Deep {Mesh} {Reconstruction} {From} {Single} {RGB} {Images} via {Topology} {Modification} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009447/},
	doi = {10.1109/ICCV.2019.01006},
	abstract = {Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difﬁculty of generating a feasible mesh structure, the stateof-the-art approaches [16, 32] often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difﬁcult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modiﬁcation. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modiﬁcation network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary reﬁnement network is designed to reﬁne the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Pan, Junyi and Han, Xiaoguang and Chen, Weikai and Tang, Jiapeng and Jia, Kui},
	month = oct,
	year = {2019},
	pages = {9963--9972},
}

@inproceedings{zhang_end--end_2019,
	address = {Seoul, Korea (South)},
	title = {End-to-{End} {Hand} {Mesh} {Recovery} {From} a {Monocular} {RGB} {Image}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008830/},
	doi = {10.1109/ICCV.2019.00244},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Xiong and Li, Qiang and Mo, Hong and Zhang, Wenbo and Zheng, Wen},
	month = oct,
	year = {2019},
	pages = {2354--2364},
}

@article{lin_learning_2018,
	title = {Learning {Efficient} {Point} {Cloud} {Generation} for {Dense} {3D} {Object} {Reconstruction}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12278},
	language = {en},
	number = {1},
	urldate = {2021-03-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Chen-Hsuan and Kong, Chen and Lucey, Simon},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {generative modeling},
}

@article{chu_generative_2019,
	title = {Generative {Adversarial} {Network}-{Based} {Method} for {Transforming} {Single} {RGB} {Image} {Into} {3D} {Point} {Cloud}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2886213},
	abstract = {Three-dimensional (3D) point clouds are important for many applications, including object tracking and 3D scene reconstruction. Point clouds are usually obtained from laser scanners, but their high cost impedes the widespread adoption of this technology. We propose a method to generate the 3D point cloud corresponding to a single red–green–blue (RGB) image. The method retrieves high-quality 3D data from two-dimensional (2D) images captured by conventional cameras, which are generally less expensive. The proposed method comprises two stages. First, a generative adversarial network generates a depth image estimation from a single RGB image. Then, the 3D point cloud is calculated from the depth image. The estimation relies on the parameters of the depth camera employed to generate the training data. The experimental results verify that the proposed method provides high-quality 3D point clouds from single 2D images. Moreover, the method does not require a PC with outstanding computational resources, further reducing implementation costs, as only a moderate-capacity graphics processing unit can efficiently handle the calculations.},
	journal = {IEEE Access},
	author = {Chu, P. M. and Sung, Y. and Cho, K.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Cameras, Gallium nitride, Generative adversarial networks, Generators, Three-dimensional displays, Training, Two dimensional displays, image processing, machine learning, neural networks, sensors},
	pages = {1021--1029},
}

@article{fan_pointrnn_2019,
	title = {{PointRNN}: {Point} {Recurrent} {Neural} {Network} for {Moving} {Point} {Cloud} {Processing}},
	shorttitle = {{PointRNN}},
	url = {http://arxiv.org/abs/1910.08287},
	abstract = {In this paper, we introduce a Point Recurrent Neural Network (PointRNN) for moving point cloud processing. At each time step, PointRNN takes point coordinates \${\textbackslash}boldsymbol\{P\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n {\textbackslash}times 3\}\$ and point features \${\textbackslash}boldsymbol\{X\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n {\textbackslash}times d\}\$ as input (\$n\$ and \$d\$ denote the number of points and the number of feature channels, respectively). The state of PointRNN is composed of point coordinates \${\textbackslash}boldsymbol\{P\}\$ and point states \${\textbackslash}boldsymbol\{S\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n {\textbackslash}times d'\}\$ (\$d'\$ denotes the number of state channels). Similarly, the output of PointRNN is composed of \${\textbackslash}boldsymbol\{P\}\$ and new point features \${\textbackslash}boldsymbol\{Y\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n {\textbackslash}times d''\}\$ (\$d''\$ denotes the number of new feature channels). Since point clouds are orderless, point features and states from two time steps can not be directly operated. Therefore, a point-based spatiotemporally-local correlation is adopted to aggregate point features and states according to point coordinates. We further propose two variants of PointRNN, i.e., Point Gated Recurrent Unit (PointGRU) and Point Long Short-Term Memory (PointLSTM). We apply PointRNN, PointGRU and PointLSTM to moving point cloud prediction, which aims to predict the future trajectories of points in a set given their history movements. Experimental results show that PointRNN, PointGRU and PointLSTM are able to produce correct predictions on both synthetic and real-world datasets, demonstrating their ability to model point cloud sequences. The code has been released at {\textbackslash}url\{https://github.com/hehefan/PointRNN\}.},
	urldate = {2021-03-25},
	journal = {arXiv:1910.08287 [cs]},
	author = {Fan, Hehe and Yang, Yi},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.08287},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{ferrari_pixel2mesh_2018,
	address = {Cham},
	title = {{Pixel2Mesh}: {Generating} {3D} {Mesh} {Models} from {Single} {RGB} {Images}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	shorttitle = {{Pixel2Mesh}},
	url = {http://link.springer.com/10.1007/978-3-030-01252-6_4},
	abstract = {We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-ﬁne strategy to make the whole deformation procedure stable, and deﬁne various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {55--71},
}

@inproceedings{zhang_end--end_2019-1,
	address = {Seoul, Korea (South)},
	title = {End-to-{End} {Hand} {Mesh} {Recovery} {From} a {Monocular} {RGB} {Image}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008830/},
	doi = {10.1109/ICCV.2019.00244},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Xiong and Li, Qiang and Mo, Hong and Zhang, Wenbo and Zheng, Wen},
	month = oct,
	year = {2019},
	pages = {2354--2364},
}

@article{orts-escolano_3d_2016,
	title = {{3D} {Surface} {Reconstruction} of {Noisy} {Point} {Clouds} {Using} {Growing} {Neural} {Gas}: {3D} {Object}/{Scene} {Reconstruction}},
	volume = {43},
	issn = {1370-4621, 1573-773X},
	shorttitle = {{3D} {Surface} {Reconstruction} of {Noisy} {Point} {Clouds} {Using} {Growing} {Neural} {Gas}},
	url = {http://link.springer.com/10.1007/s11063-015-9421-x},
	doi = {10.1007/s11063-015-9421-x},
	abstract = {With the advent of low-cost 3D sensors and 3D printers, scene and object 3D surface reconstruction has become an important research topic in the last years. In this work, we propose an automatic (unsupervised) method for 3D surface reconstruction from raw unorganized point clouds acquired using low-cost 3D sensors. We have modiﬁed the growing neural gas network, which is a suitable model because of its ﬂexibility, rapid adaptation and excellent quality of representation, to perform 3D surface reconstruction of different real-world objects and scenes. Some improvements have been made on the original algorithm considering colour and surface normal information of input data during the learning stage and creating complete triangular meshes instead of basic wire-frame representations. The proposed method is able to successfully create 3D faces online, whereas existing 3D reconstruction methods based on self-organizing maps required post-processing steps to close gaps and holes produced during the 3D reconstruction process. A set of quantitative and qualitative experiments were carried out to validate the proposed method. The method has been implemented and tested on real data, and has been found to be effective at reconstructing noisy point clouds obtained using low-cost 3D sensors.},
	language = {en},
	number = {2},
	urldate = {2021-03-23},
	journal = {Neural Processing Letters},
	author = {Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Morell, Vicente and Cazorla, Miguel and Perez, Jose Antonio Serra and Garcia-Garcia, Alberto},
	month = apr,
	year = {2016},
	pages = {401--423},
}

@article{lim_surface_2014,
	title = {Surface reconstruction techniques: a review},
	volume = {42},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Surface reconstruction techniques},
	url = {http://link.springer.com/10.1007/s10462-012-9329-z},
	doi = {10.1007/s10462-012-9329-z},
	abstract = {Surface reconstruction means that retrieve the data by scanning an object using a device such as laser scanner and construct it using the computer to gain back the soft copy of data on that particular object. It is a reverse process and is very useful especially when that particular object original data is missing without doing any backup. Hence, by doing so, the data can be recollected and can be stored for future purposes. The type of data can be in the form of structure or unstructured points. The accuracy of the reconstructed result should be concerned because if the result is incorrect, hence it will not exactly same like the original shape of the object. Therefore, suitable methods should be chosen based on the data used. Soft computing methods also have been used in the reconstruction ﬁeld. This papers highlights the previous researches and methods that has been used in the surface reconstruction ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2021-03-23},
	journal = {Artificial Intelligence Review},
	author = {Lim, Seng Poh and Haron, Habibollah},
	month = jun,
	year = {2014},
	pages = {59--78},
}

@article{kohonen_self-organizing_1990,
	title = {The self-organizing map},
	volume = {78},
	issn = {1558-2256},
	doi = {10.1109/5.58325},
	abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.{\textless}{\textgreater}},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Kohonen, T.},
	month = sep,
	year = {1990},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Animals, Artificial neural networks, Biological neural networks, Computer networks, Organizing, Pattern recognition, Process control, Signal processing, Signal processing algorithms, Speech recognition, clustering, competitive learning, learning systems, learning vector, neural nets, neural networks, self-adjusting systems, self-organizing map, semantic maps, speech recognition},
	pages = {1464--1480},
}

@article{florence_dense_2018,
	title = {Dense {Object} {Nets}: {Learning} {Dense} {Visual} {Object} {Descriptors} {By} and {For} {Robotic} {Manipulation}},
	shorttitle = {Dense {Object} {Nets}},
	url = {http://arxiv.org/abs/1806.08756},
	abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
	urldate = {2021-03-22},
	journal = {arXiv:1806.08756 [cs]},
	author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
	month = sep,
	year = {2018},
	note = {arXiv: 1806.08756},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{florence_dense_2018-1,
	title = {Dense {Object} {Nets}: {Learning} {Dense} {Visual} {Object} {Descriptors} {By} and {For} {Robotic} {Manipulation}},
	shorttitle = {Dense {Object} {Nets}},
	url = {http://arxiv.org/abs/1806.08756},
	abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
	language = {en},
	urldate = {2021-03-22},
	journal = {arXiv:1806.08756 [cs]},
	author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
	month = sep,
	year = {2018},
	note = {arXiv: 1806.08756},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{minaee_image_2021,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	doi = {10.1109/TPAMI.2021.3059968},
	abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Minaee, S. and Boykov, Y. Y. and Porikli, F. and Plaza, A. J. and Kehtarnavaz, N. and Terzopoulos, D.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer architecture, Deep learning, Generative adversarial networks, Image segmentation, Logic gates, Semantics, convolutional neural networks, deep learning, encoder-decoder models, generative models, instance segmentation, medical image segmentation, panoptic segmentation, recurrent models, semantic segmentation},
	pages = {1--1},
}

@article{ghosh_understanding_2019,
	title = {Understanding {Deep} {Learning} {Techniques} for {Image} {Segmentation}},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3329784},
	doi = {10.1145/3329784},
	abstract = {The machine learning community has been overwhelmed by a plethora of deep learning--based approaches. Many challenging computer vision tasks, such as detection, localization, recognition, and segmentation of objects in an unconstrained environment, are being efficiently addressed by various types of deep neural networks, such as convolutional neural networks, recurrent networks, adversarial networks, and autoencoders. Although there have been plenty of analytical studies regarding the object detection or recognition domain, many new deep learning techniques have surfaced with respect to image segmentation techniques. This article approaches these various deep learning techniques of image segmentation from an analytical perspective. The main goal of this work is to provide an intuitive understanding of the major techniques that have made a significant contribution to the image segmentation domain. Starting from some of the traditional image segmentation approaches, the article progresses by describing the effect that deep learning has had on the image segmentation domain. Thereafter, most of the major segmentation algorithms have been logically categorized with paragraphs dedicated to their unique contribution. With an ample amount of intuitive explanations, the reader is expected to have an improved ability to visualize the internal dynamics of these processes.},
	number = {4},
	urldate = {2021-03-19},
	journal = {ACM Computing Surveys},
	author = {Ghosh, Swarnendu and Das, Nibaran and Das, Ishita and Maulik, Ujjwal},
	month = aug,
	year = {2019},
	keywords = {Deep learning, convolutional neural networks, semantic image segmentation},
	pages = {73:1--73:35},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-03-19},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ronneberger_u-net_2015-1,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2021-03-19},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bouwmans_subspace_nodate,
	title = {Subspace learning for background modeling: {A} survey},
	shorttitle = {Subspace learning for background modeling},
	url = {https://www.academia.edu/691079/Subspace_learning_for_background_modeling_A_survey},
	language = {en},
	urldate = {2021-03-18},
	journal = {Recent Patents on Computer Science},
	author = {Bouwmans, Thierry},
}

@article{metcalfe_avalanche_1995,
	title = {Avalanche mixing of granular solids},
	volume = {374},
	copyright = {1995 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/374039a0},
	doi = {10.1038/374039a0},
	abstract = {THE production of many goods, ranging from pharmaceuticals and foods to polymers and semiconductors, depends on reliable, uniform mixing of solids. Although there have been several notable recent advances1–6, solid mixing processes are still poorly understood. We can neither qualitatively nor quantitively determine the effectiveness of any given mixing process in advance. In contrast to the case of liquid mixing7, we do not have a widely accepted theoretical basis that describes the mixing of solids. Moreover, we cannot determine whether a given set of solids will mix or separate during a specified stirring process8–23. As a step towards uncovering the basic physical principles, it is helpful to analyse systems that are both experimentally and theoretically tractable. Here we describe a geometric technique for the analysis of slow granular mixing processes, as are commonly encountered in industry. By comparing our calculations with experiments on thin rotating containers partially filled with coloured particles, we demonstrate that the mixing behaviour of powders in slow flows can be divided into geometric and dynamic parts. For monodisperse, weakly cohesive particles, geometric aspects dominate.},
	language = {en},
	number = {6517},
	urldate = {2021-03-14},
	journal = {Nature},
	author = {Metcalfe, Guy and Shinbrot, Troy and McCarthy, J. J. and Ottino, Julio M.},
	month = mar,
	year = {1995},
	note = {Number: 6517
Publisher: Nature Publishing Group},
	pages = {39--41},
}

@article{navarro_model-based_2020,
	title = {A {Model}-{Based} {Sensor} {Fusion} {Approach} for {Force} and {Shape} {Estimation} in {Soft} {Robotics}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.3008120},
	abstract = {In this letter, we address the challenge of sensor fusion in Soft Robotics for estimating forces and deformations. In the context of intrinsic sensing, we propose the use of soft capacitive sensing to find a contact's location, and the use of pneumatic sensing to estimate the force intensity and the deformation. Using a FEM-based numerical approach, we integrate both sensing streams and model two Soft Robotics devices we have conceived. These devices are a Soft Pad and a Soft Finger. We show in an evaluation that external forces on the Soft Pad can be estimated and that the shape of the Soft Finger can be reconstructed.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Navarro, S. E. and Nagels, S. and Alagi, H. and Faller, L. and Goury, O. and Morales-Bieze, T. and Zangl, H. and Hein, B. and Ramakers, R. and Deferme, W. and Zheng, G. and Duriez, C.},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Electrodes, FEM-based numerical approach, Fingers, Modeling, control, and learning for soft robots, Robot sensing systems, Shape, Soft Finger, Soft Pad, Soft Robotics, Soft robotics, Strain, deformations, external forces, finite element analysis, force and tactile sensing, force intensity, intrinsic sensing, medical robotics, model-based sensor fusion approach, pneumatic sensing, pneumatic systems, sensing streams, sensor fusion, soft capacitive sensing, soft robot materials and design, soft sensors and actuators},
	pages = {5621--5628},
}

@inproceedings{geiger_stereoscan_2011,
	title = {{StereoScan}: {Dense} 3d reconstruction in real-time},
	shorttitle = {{StereoScan}},
	doi = {10.1109/IVS.2011.5940405},
	abstract = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
	booktitle = {2011 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Geiger, A. and Ziegler, J. and Stiller, C.},
	month = jun,
	year = {2011},
	note = {ISSN: 1931-0587},
	keywords = {3D maps, 3D point clouds, Cameras, Estimation, Image reconstruction, Real time systems, Stereo image processing, StereoScan, Three dimensional displays, Visualization, camera resolution, computer vision, dense 3D reconstruction, distance measurement, feature matcher, high resolution stereo sequences, image matching, image reconstruction, image sequences, multiview linking scheme, robot vision, robotics, robust visual odometry algorithm, scene analysis, stereo image processing, stereo matching, video sequences},
	pages = {963--968},
}

@inproceedings{bern_soft_2020,
	title = {Soft {Robot} {Control} {With} a {Learned} {Differentiable} {Model}},
	doi = {10.1109/RoboSoft48309.2020.9116011},
	abstract = {Soft robots are inherently safe and comply readily to their environment. They are therefore exciting for applications like search and rescue or medicine, which involve a high degree of uncertainty, and require interacting with humans. However, the best way to model and control soft robots largely remains an open question. One promising approach is to leverage physically-based modeling techniques such as the finite element method. However, such techniques are inherently limited by their physical assumptions. Indeed, real-world soft robots are often made from unpredictable materials, using imprecise techniques. Data-driven approaches provide an exciting alternative, as they can learn real-world fabrication defects and asymmetries. In this paper we present our first investigation into using machine learning to do soft robot control. We learn a differentiable model of a soft robot's quasi-static physics, and then perform gradient-based optimization to find optimal open-loop control inputs. We find that our learned model captures phenomena that would be absent from an idealized physically-based simulation. We also present practical techniques for acquiring high-quality motion capture data, and observations the effect of network complexity on model accuracy.},
	booktitle = {2020 3rd {IEEE} {International} {Conference} on {Soft} {Robotics} ({RoboSoft})},
	author = {Bern, J. M. and Schnider, Y. and Banzet, P. and Kumar, N. and Coros, S.},
	month = may,
	year = {2020},
	keywords = {Control, Learning, Modeling, Soft robots, control engineering computing, data-driven approach, finite element analysis, finite element method, gradient methods, human-robot interaction, learned differentiable model, learning (artificial intelligence), machine learning, motion control, network complexity, open loop systems, optimal open-loop control, optimisation, real-world fabrication defects, robots, soft robot control},
	pages = {417--423},
}

@article{loper_mosh_2014,
	title = {{MoSh}: motion and shape capture from sparse markers},
	volume = {33},
	issn = {0730-0301},
	shorttitle = {{MoSh}},
	url = {https://doi.org/10.1145/2661229.2661273},
	doi = {10.1145/2661229.2661273},
	abstract = {Marker-based motion capture (mocap) is widely criticized as producing lifeless animations. We argue that important information about body surface motion is present in standard marker sets but is lost in extracting a skeleton. We demonstrate a new approach called MoSh (Motion and Shape capture), that automatically extracts this detail from mocap data. MoSh estimates body shape and pose together using sparse marker data by exploiting a parametric model of the human body. In contrast to previous work, MoSh solves for the marker locations relative to the body and estimates accurate body shape directly from the markers without the use of 3D scans; this effectively turns a mocap system into an approximate body scanner. MoSh is able to capture soft tissue motions directly from markers by allowing body shape to vary over time. We evaluate the effect of different marker sets on pose and shape accuracy and propose a new sparse marker set for capturing soft-tissue motion. We illustrate MoSh by recovering body shape, pose, and soft-tissue motion from archival mocap data and using this to produce animations with subtlety and realism. We also show soft-tissue motion retargeting to new characters and show how to magnify the 3D deformations of soft tissue to create animations with appealing exaggerations.},
	number = {6},
	urldate = {2021-03-01},
	journal = {ACM Transactions on Graphics},
	author = {Loper, Matthew and Mahmood, Naureen and Black, Michael J.},
	month = nov,
	year = {2014},
	keywords = {human animation, motion capture, shape capture, soft tissue motion},
	pages = {220:1--220:13},
}

@article{loper_mosh_2014-1,
	title = {{MoSh}: motion and shape capture from sparse markers},
	volume = {33},
	issn = {0730-0301},
	shorttitle = {{MoSh}},
	url = {https://doi.org/10.1145/2661229.2661273},
	doi = {10.1145/2661229.2661273},
	abstract = {Marker-based motion capture (mocap) is widely criticized as producing lifeless animations. We argue that important information about body surface motion is present in standard marker sets but is lost in extracting a skeleton. We demonstrate a new approach called MoSh (Motion and Shape capture), that automatically extracts this detail from mocap data. MoSh estimates body shape and pose together using sparse marker data by exploiting a parametric model of the human body. In contrast to previous work, MoSh solves for the marker locations relative to the body and estimates accurate body shape directly from the markers without the use of 3D scans; this effectively turns a mocap system into an approximate body scanner. MoSh is able to capture soft tissue motions directly from markers by allowing body shape to vary over time. We evaluate the effect of different marker sets on pose and shape accuracy and propose a new sparse marker set for capturing soft-tissue motion. We illustrate MoSh by recovering body shape, pose, and soft-tissue motion from archival mocap data and using this to produce animations with subtlety and realism. We also show soft-tissue motion retargeting to new characters and show how to magnify the 3D deformations of soft tissue to create animations with appealing exaggerations.},
	number = {6},
	urldate = {2021-03-01},
	journal = {ACM Transactions on Graphics},
	author = {Loper, Matthew and Mahmood, Naureen and Black, Michael J.},
	month = nov,
	year = {2014},
	keywords = {human animation, motion capture, shape capture, soft tissue motion},
	pages = {220:1--220:13},
}

@article{guan_deep_2019,
	title = {Deep convolutional neural network {VGG}-16 model for differential diagnosing of papillary thyroid carcinomas in cytological images: a pilot study},
	volume = {10},
	issn = {1837-9664},
	shorttitle = {Deep convolutional neural network {VGG}-16 model for differential diagnosing of papillary thyroid carcinomas in cytological images},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6775529/},
	doi = {10.7150/jca.28769},
	abstract = {Objective: In this study, we exploited a VGG-16 deep convolutional neural network (DCNN) model to differentiate papillary thyroid carcinoma (PTC) from benign thyroid nodules using cytological images., Methods: A pathology-proven dataset was built from 279 cytological images of thyroid nodules. The images were cropped into fragmented images and divided into a training dataset and a test dataset. VGG-16 and Inception-v3 DCNNs were trained and tested to make differential diagnoses. The characteristics of tumor cell nucleus were quantified as contours, perimeter, area and mean of pixel intensity and compared using independent Student's t-tests., Results: In the test group, the accuracy rates of the VGG-16 model and Inception-v3 on fragmented images were 97.66\% and 92.75\%, respectively, and the accuracy rates of VGG-16 and Inception-v3 in patients were 95\% and 87.5\%, respectively. The contours, perimeter, area and mean of pixel intensity of PTC in fragmented images were more than the benign nodules, which were 61.01±17.10 vs 47.00±24.08, p=0.000, 134.99±21.42 vs 62.40±29.15, p=0.000, 1770.89±627.22 vs 1157.27±722.23, p=0.013, 165.84±26.33 vs 132.94±28.73, p=0.000), respectively., Conclusion: In summary, after training with a large dataset, the DCNN VGG-16 model showed great potential in facilitating PTC diagnosis from cytological images. The contours, perimeter, area and mean of pixel intensity of PTC in fragmented images were more than the benign nodules.},
	number = {20},
	urldate = {2021-02-28},
	journal = {Journal of Cancer},
	author = {Guan, Qing and Wang, Yunjun and Ping, Bo and Li, Duanshu and Du, Jiajun and Qin, Yu and Lu, Hongtao and Wan, Xiaochun and Xiang, Jun},
	month = aug,
	year = {2019},
	pmid = {31598159},
	pmcid = {PMC6775529},
	pages = {4876--4882},
}

@inproceedings{liu_classifying_2018,
	title = {Classifying {High} {Resolution} {Remote} {Sensing} {Images} by {Fine}-{Tuned} {VGG} {Deep} {Networks}},
	doi = {10.1109/IGARSS.2018.8518078},
	abstract = {Deep convolutional networks perform well in remote sensing (RS) image classification. Usually, it is difficult to obtain a large number of labeled samples in remote sensing classification tasks. Traditionally, the acquisition of remote sensing images is quite different from the photos provided by digital cameras. However, the imaging system for high resolution (HR) RS images (often with RGB 3 channels) is similar to those provided by digital cameras. In the paper, a transfer learning algorithm based on deep neural networks is proposed to attack the problem of lacking labeled RS samples, in particular on the context of pre-trained deep convolutional networks, i.e., VGGNet. Here, the VGGNet is trained on labeled multimedia images provided by “ImageNet Large Scale Visual Recognition Challenge” (ILSVRC). In the proposed strategy, the VGGNet is adopted as a base classifier, and then labeled RS data samples are exploited to fine-tune higher hidden layers in the 16-layer VGG deep neural networks by the back-propagation algorithm. The proposed method is denoted as RS-VGGNet. The proposed RS-VGGNet is validated by real HR remote sensing images, which were acquired from the National Agriculture Imagery Program(NAIP) dataset. Experimental results show that the RS-VGGNet can achieve a higher accuracy compared to the original VGGNet and shallow machine learning methods. And the proposed RS-VGGNet significantly reduces training times and computing burden as well.},
	booktitle = {{IGARSS} 2018 - 2018 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Liu, X. and Chi, M. and Zhang, Y. and Qin, Y.},
	month = jul,
	year = {2018},
	note = {ISSN: 2153-7003},
	keywords = {Convolution, Fine-tuning VGG Net, HR remote sensing images, Image resolution, ImageNet Large Scale Visual Recognition Challenge, Machine learning, Neural networks, RGB 3 channels, RS-VGGNet, Remote sensing, Task analysis, Training, VGG deep neural networks, back-propagation algorithm, backpropagation, base classifier, convolution, deep convolutional networks, deep neural network, digital cameras, feedforward neural nets, fine-tuned VGG deep networks, geophysical image processing, high resolution RS images, high resolution remote sensing image, high resolution remote sensing images, image classification, image colour analysis, image resolution, imaging system, labeled multimedia images, remote sensing, remote sensing classification tasks, remote sensing image classification, transfer learning, transfer learning algorithm},
	pages = {7137--7140},
}

@article{qawaqneh_deep_2017,
	title = {Deep {Convolutional} {Neural} {Network} for {Age} {Estimation} based on {VGG}-{Face} {Model}},
	url = {http://arxiv.org/abs/1709.01664},
	abstract = {Automatic age estimation from real-world and unconstrained face images is rapidly gaining importance. In our proposed work, a deep CNN model that was trained on a database for face recognition task is used to estimate the age information on the Adience database. This paper has three significant contributions in this field. (1) This work proves that a CNN model, which was trained for face recognition task, can be utilized for age estimation to improve performance; (2) Over fitting problem can be overcome by employing a pretrained CNN on a large database for face recognition task; (3) Not only the number of training images and the number subjects in a training database effect the performance of the age estimation model, but also the pre-training task of the employed CNN determines the performance of the model.},
	urldate = {2021-02-28},
	journal = {arXiv:1709.01664 [cs]},
	author = {Qawaqneh, Zakariya and Mallouh, Arafat Abu and Barkana, Buket D.},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01664},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_real-time_2020,
	title = {Real-{Time} {Soft} {Body} {3D} {Proprioception} via {Deep} {Vision}-{Based} {Sensing}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2975709},
	abstract = {Soft bodies made from flexible and deformable materials are popular in many robotics applications, but their proprioceptive sensing has been a long-standing challenge. In other words, there has hardly been a method to measure and model the high-dimensional 3D shapes of soft bodies with internal sensors. We propose a framework to measure the high-resolution 3D shapes of soft bodies in real-time with embedded cameras. The cameras capture visual patterns inside a soft body, and a convolutional neural network (CNN) produces a latent code representing the deformation state, which can then be used to reconstruct the body's 3D shape using another neural network. We test the framework on various soft bodies, such as a Baymax-shaped toy, a latex balloon, and some soft robot fingers, and achieve real-time computation (≤2.5 ms/frame) for robust shape estimation with high precision (≤1\% relative error) and high resolution. We believe the method could be applied to soft robotics and human-robot interaction for proprioceptive shape sensing. Our code is available at: https: //ai4ce.github.io/DeepSoRo/.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, R. and Wang, S. and Du, S. and Xiao, E. and Yuan, W. and Feng, C.},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {3D deep learning, Baymax-shaped toy, Modeling, control, and learning for soft robots, Robot sensing systems, Shape, Soft robotics, Strain, Three-dimensional displays, cameras, computer vision, convolutional neural nets, deep learning in robotics and automation, high-dimensional 3D shapes, high-resolution 3D shapes, human-robot interaction, image reconstruction, image representation, image resolution, image sensors, latex balloon, mobile robots, proprioceptive shape sensing, robot vision, robust shape estimation, shape recognition, soft body 3D proprioception, soft robot fingers, soft robotics},
	pages = {3382--3389},
}

@article{abondance_dexterous_2020,
	title = {A {Dexterous} {Soft} {Robotic} {Hand} for {Delicate} {In}-{Hand} {Manipulation}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.3007411},
	abstract = {In this letter, we show that soft robotic hands provide a robust means of performing basic primitives of in-hand manipulation in the presence of uncertainty. We first discuss the design of a prototype hand with dexterous soft fingers capable of moving objects within the hand using several basic motion primitives. We then empirically validate the ability of the hand to perform the desired object motion primitives while still maintaining strong grasping capabilities. Based on these primitives, we examine a simple, heuristic finger gait which enables continuous object rotation for a wide variety of object shapes and sizes. Finally, we demonstrate the utility of our dexterous soft robotic hand in three real-world cases: unscrewing the cap of a jar, orienting food items for packaging, and gravity compensation during grasping. Overall, we show that even for complex tasks such as in-hand manipulation, soft robots can perform robustly without the need for local sensing or complex control.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Abondance, S. and Teeple, C. B. and Wood, R. J.},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Actuators, Grasping, In-hand manipulation, Shape, Soft robotics, Task analysis, Uncertainty, contact dynamics, continuous object rotation, delicate in-hand manipulation, dexterous manipulation, dexterous manipulators, dexterous soft fingers, dexterous soft robotic hand, grasping capabilities, grippers, heuristic finger gait, manipulator dynamics, motion control, object motion, object shapes, robot dynamics, soft robot applications},
	pages = {5502--5509},
}

@article{lin_automatic_2020,
	title = {Automatic {3D} color shape measurement system based on a stereo camera},
	volume = {59},
	copyright = {\&\#169; 2020 Optical Society of America},
	issn = {2155-3165},
	url = {https://www.osapublishing.org/ao/abstract.cfm?uri=ao-59-7-2086},
	doi = {10.1364/AO.384222},
	abstract = {This study proposes an automatic three-dimensional (3D) color shape measurement system based on images recorded by a stereo camera. The system, comprising several off-the-shelf components, is cost-effective yet capable of obtaining quality color 3D objects. In the proposed system, a turntable carrying a checkerboard is used to assist the simultaneous calibration of the stereo camera and the turntable. A slit laser is driven to swing forward and backward for generating stripes on test objects. The stereo images are collected and analyzed for obtaining matching pixels, and, consequently, the 3D point coordinates based on epipolar geometry are obtained. Screened Poisson reconstruction is utilized to integrate and smooth the scanned surfaces. With additional color images from the same camera, several multi-view texturing methods are benchmarked. We concluded that our proposed system can successfully and automatically reconstruct quality 3D color shapes of various objects.},
	language = {EN},
	number = {7},
	urldate = {2021-02-28},
	journal = {Applied Optics},
	author = {Lin, Tzung-Han},
	month = mar,
	year = {2020},
	note = {Publisher: Optical Society of America},
	keywords = {Camera calibration, Laser beams, Laser scanning, Laser triangulation, Optical components, Position sensing equipment},
	pages = {2086--2096},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.osapublishing.org/view_article.cfm?gotourl=https%3A%2F%2Fwww%2Eosapublishing%2Eorg%2FDirectPDFAccess%2F20676C4A%2D8786%2D4B36%2D9099449274D5F902%5F427784%2Fao%2D59%2D7%2D2086%2Epdf%3Fda%3D1%26id%3D427784%26shib%3D578835%26seq%3D0%26mobile%3Dno&org=Eidgenossische%20Technische%20Hochschule%20Zurich%20Bibliothek%20%2D%20ETH%20Zurich},
	urldate = {2021-02-28},
}

@misc{noauthor_notitle_nodate-1,
	url = {https://www.osapublishing.org/view_article.cfm?gotourl=https%3A%2F%2Fwww%2Eosapublishing%2Eorg%2FDirectPDFAccess%2F20676C4A%2D8786%2D4B36%2D9099449274D5F902%5F427784%2Fao%2D59%2D7%2D2086%2Epdf%3Fda%3D1%26id%3D427784%26shib%3D578835%26seq%3D0%26mobile%3Dno&org=Eidgenossische%20Technische%20Hochschule%20Zurich%20Bibliothek%20%2D%20ETH%20Zurich},
	urldate = {2021-02-28},
}

@misc{noauthor_notitle_nodate-2,
	url = {https://www.osapublishing.org/view_article.cfm?gotourl=https%3A%2F%2Fwww%2Eosapublishing%2Eorg%2FDirectPDFAccess%2F20676C4A%2D8786%2D4B36%2D9099449274D5F902%5F427784%2Fao%2D59%2D7%2D2086%2Epdf%3Fda%3D1%26id%3D427784%26shib%3D578835%26seq%3D0%26mobile%3Dno&org=Eidgenossische%20Technische%20Hochschule%20Zurich%20Bibliothek%20%2D%20ETH%20Zurich},
	urldate = {2021-02-28},
}

@inproceedings{li_marker-based_2020,
	title = {Marker-{Based} {Shape} {Estimation} of a {Continuum} {Manipulator} {Using} {Binocular} {Vision} and {Its} {Error} {Compensation}},
	doi = {10.1109/ICMA49215.2020.9233616},
	abstract = {Robot-assisted laparoscopy endoscopic single-site surgery robot (LESS) procedures typically involve highly flexible and continuum manipulators to get access to the surgical targets through a small keyhole incision and then perform complicated operations within the abdomen or chest cavity. However, the inherent deformable design and inevitable collisions with the anatomy cause both active and passive deformation of the distal continuum manipulator, which challenges the accurate shape estimation in both structured and unstructured environment. Therefore, it is essential to achieve real-time and accurate and shape tracking of the continuum manipulator for precise and reliable motion control. A new method is proposed in this paper for solving in the problem of real-time shape sensing for the distal continuous robot using binoculars by extracting position information of each markers. However, to cope with the problems of visual occlusion and limited algorithm stability, an error compensation method based on machine learning algorithm is realized. The performance of the proposed algorithm was verified by experiments, and the accuracy of the reconstruction after the correction by the learning algorithm was improved by about 70\%.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	author = {Li, J. and Sun, Y. and Su, H. and Zhang, G. and Shi, C.},
	month = oct,
	year = {2020},
	note = {ISSN: 2152-744X},
	keywords = {Binocular vision, Continuum manipulator, LESS/SPL, MIS, Machine learning, Shape estimation, abdomen, binocular vision, binoculars, deformation, distal continuous robot, distal continuum manipulator, endoscopes, error compensation, error compensation method, inevitable collisions, inherent deformable design, learning (artificial intelligence), machine learning algorithm, manipulators, marker-based shape estimation, medical image processing, medical robotics, motion control, passive deformation, position control, precise motion control, real-time shape sensing, reliable motion control, robot-assisted laparoscopy endoscopic single-site surgery robot procedures, shape estimation, shape tracking, structured environment, surgery, surgical targets},
	pages = {1745--1750},
}

@article{dhillon_convolutional_2020,
	title = {Convolutional neural network: a review of models, methodologies and applications to object detection},
	volume = {9},
	issn = {2192-6360},
	shorttitle = {Convolutional neural network},
	url = {https://doi.org/10.1007/s13748-019-00203-0},
	doi = {10.1007/s13748-019-00203-0},
	abstract = {Deep learning has developed as an effective machine learning method that takes in numerous layers of features or representation of the data and provides state-of-the-art results. The application of deep learning has shown impressive performance in various application areas, particularly in image classification, segmentation and object detection. Recent advances of deep learning techniques bring encouraging performance to fine-grained image classification which aims to distinguish subordinate-level categories. This task is extremely challenging due to high intra-class and low inter-class variance. In this paper, we provide a detailed review of various deep architectures and model highlighting characteristics of particular model. Firstly, we described the functioning of CNN architectures and its components followed by detailed description of various CNN models starting with classical LeNet model to AlexNet, ZFNet, GoogleNet, VGGNet, ResNet, ResNeXt, SENet, DenseNet, Xception, PNAS/ENAS. We mainly focus on the application of deep learning architectures to three major applications, namely (i) wild animal detection, (ii) small arm detection and (iii) human being detection. A detailed review summary including the systems, database, application and accuracy claimed is also provided for each model to serve as guidelines for future work in the above application areas.},
	language = {en},
	number = {2},
	urldate = {2021-02-28},
	journal = {Progress in Artificial Intelligence},
	author = {Dhillon, Anamika and Verma, Gyanendra K.},
	month = jun,
	year = {2020},
	pages = {85--112},
}

@article{wang_toward_2018,
	title = {Toward {Perceptive} {Soft} {Robots}: {Progress} and {Challenges}},
	volume = {5},
	copyright = {© 2018 The Authors. Published by WILEY‐VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {2198-3844},
	shorttitle = {Toward {Perceptive} {Soft} {Robots}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.201800541},
	doi = {https://doi.org/10.1002/advs.201800541},
	abstract = {In the past few years, soft robotics has rapidly become an emerging research topic, opening new possibilities for addressing real-world tasks. Perception can enable robots to effectively explore the unknown world, and interact safely with humans and the environment. Among all extero- and proprioception modalities, the detection of mechanical cues is vital, as with living beings. A variety of soft sensing technologies are available today, but there is still a gap to effectively utilize them in soft robots for practical applications. Here, the developments in soft robots with mechanical sensing are summarized to provide a comprehensive understanding of the state of the art in this field. Promising sensing technologies for mechanically perceptive soft robots are described, categorized, and their pros and cons are discussed. Strategies for designing soft sensors and criteria to evaluate their performance are outlined from the perspective of soft robotic applications. Challenges and trends in developing multimodal sensors, stretchable conductive materials and electronic interfaces, modeling techniques, and data interpretation for soft robotic sensing are highlighted. The knowledge gap and promising solutions toward perceptive soft robots are discussed and analyzed to provide a perspective in this field.},
	language = {en},
	number = {9},
	urldate = {2021-02-20},
	journal = {Advanced Science},
	author = {Wang, Hongbo and Totaro, Massimo and Beccai, Lucia},
	year = {2018},
	keywords = {proprioception, robotic sensing, soft robotics, soft sensors, tactile sensing},
	pages = {1800541},
}

@article{polygerinos_soft_2017,
	title = {Soft {Robotics}: {Review} of {Fluid}-{Driven} {Intrinsically} {Soft} {Devices}; {Manufacturing}, {Sensing}, {Control}, and {Applications} in {Human}-{Robot} {Interaction}},
	volume = {19},
	copyright = {© 2017 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {1527-2648},
	shorttitle = {Soft {Robotics}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/adem.201700016},
	doi = {https://doi.org/10.1002/adem.201700016},
	abstract = {The emerging field of soft robotics makes use of many classes of materials including metals, low glass transition temperature (Tg) plastics, and high Tg elastomers. Dependent on the specific design, all of these materials may result in extrinsically soft robots. Organic elastomers, however, have elastic moduli ranging from tens of megapascals down to kilopascals; robots composed of such materials are intrinsically soft − they are always compliant independent of their shape. This class of soft machines has been used to reduce control complexity and manufacturing cost of robots, while enabling sophisticated and novel functionalities often in direct contact with humans. This review focuses on a particular type of intrinsically soft, elastomeric robot − those powered via fluidic pressurization.},
	language = {en},
	number = {12},
	urldate = {2021-02-20},
	journal = {Advanced Engineering Materials},
	author = {Polygerinos, Panagiotis and Correll, Nikolaus and Morin, Stephen A. and Mosadegh, Bobak and Onal, Cagdas D. and Petersen, Kirstin and Cianchetti, Matteo and Tolley, Michael T. and Shepherd, Robert F.},
	year = {2017},
	pages = {1700016},
}

@article{oshea_introduction_2015,
	title = {An {Introduction} to {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.08458},
	abstract = {The ﬁeld of machine learning has taken a dramatic twist in recent times, with the rise of the Artiﬁcial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artiﬁcial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difﬁcult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simpliﬁed method of getting started with ANNs.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1511.08458 [cs]},
	author = {O'Shea, Keiron and Nash, Ryan},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.08458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-02-26},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@inproceedings{yang_foldingnet_2018,
	address = {Salt Lake City, UT},
	title = {{FoldingNet}: {Point} {Cloud} {Auto}-{Encoder} via {Deep} {Grid} {Deformation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{FoldingNet}},
	url = {https://ieeexplore.ieee.org/document/8578127/},
	doi = {10.1109/CVPR.2018.00029},
	abstract = {Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised semantic learning tasks on point clouds such as classiﬁcation and segmentation. In this work, a novel endto-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel foldingbased approach is proposed in the decoder, which folds a 2D grid onto the underlying 3D object surface of a point cloud. The proposed decoder only uses about 7\% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classiﬁcation accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Finally, this folding-based decoder is interpretable since the reconstruction could be viewed as a ﬁne granular warping from the 2D grid to the point cloud surface.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
	month = jun,
	year = {2018},
	pages = {206--215},
}

@article{kohonen_self-organizing_1990-1,
	title = {The self-organizing map},
	volume = {78},
	issn = {1558-2256},
	doi = {10.1109/5.58325},
	abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.{\textless}{\textgreater}},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Kohonen, T.},
	month = sep,
	year = {1990},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Animals, Artificial neural networks, Biological neural networks, Computer networks, Organizing, Pattern recognition, Process control, Signal processing, Signal processing algorithms, Speech recognition, clustering, competitive learning, learning systems, learning vector, neural nets, neural networks, self-adjusting systems, self-organizing map, semantic maps, speech recognition},
	pages = {1464--1480},
}

@article{zhao_optoelectronically_2016,
	title = {Optoelectronically innervated soft prosthetic hand via stretchable optical waveguides},
	volume = {1},
	issn = {2470-9476},
	url = {https://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aai7529},
	doi = {10.1126/scirobotics.aai7529},
	language = {en},
	number = {1},
	urldate = {2021-02-26},
	journal = {Science Robotics},
	author = {Zhao, Huichan and O’Brien, Kevin and Li, Shuo and Shepherd, Robert F.},
	month = dec,
	year = {2016},
	pages = {eaai7529},
}

@article{meerbeek_soft_2018,
	title = {Soft optoelectronic sensory foams with proprioception},
	volume = {3},
	copyright = {Copyright © 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {2470-9476},
	url = {https://robotics.sciencemag.org/content/3/24/eaau2489},
	doi = {10.1126/scirobotics.aau2489},
	abstract = {In a step toward soft robot proprioception, and therefore better control, this paper presents an internally illuminated elastomer foam that has been trained to detect its own deformation through machine learning techniques. Optical fibers transmitted light into the foam and simultaneously received diffuse waves from internal reflection. The diffuse reflected light was interpreted by machine learning techniques to predict whether the foam was twisted clockwise, twisted counterclockwise, bent up, or bent down. Machine learning techniques were also used to predict the magnitude of the deformation type. On new data points, the model predicted the type of deformation with 100\% accuracy and the magnitude of the deformation with a mean absolute error of 0.06°. This capability may impart soft robots with more complete proprioception, enabling them to be reliably controlled and responsive to external stimuli.
An internally illuminated elastomeric foam detects deformations through machine learning techniques.
An internally illuminated elastomeric foam detects deformations through machine learning techniques.},
	language = {en},
	number = {24},
	urldate = {2021-02-26},
	journal = {Science Robotics},
	author = {Meerbeek, I. M. Van and Sa, C. M. De and Shepherd, R. F.},
	month = nov,
	year = {2018},
	note = {Publisher: Science Robotics
Section: Research Article},
}

@article{otsu_threshold_1979,
	title = {A {Threshold} {Selection} {Method} from {Gray}-{Level} {Histograms}},
	volume = {9},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1979.4310076},
	number = {1},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Otsu, N.},
	month = jan,
	year = {1979},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics},
	keywords = {Displays, Gaussian distribution, Histograms, Least squares approximation, Marine vehicles, Q measurement, Radar tracking, Sea measurements, Surveillance, Target tracking},
	pages = {62--66},
}

@inproceedings{othman_image_2010,
	title = {Image thresholding using neural network},
	doi = {10.1109/ISDA.2010.5687030},
	abstract = {Image thresholding is a very important phase in the image analysis process. However, different images have different characteristics making the traditional process of thresholding by one algorithm a very challenging task. That is because any thresholding method may be perform well for some images but for sure it will not be suitable for all images. In this paper, intelligent thresholding by training a neural network is proposed. The neural network is trained using a set of features extracted from medical images randomly selected form a sample set and then tested using the remaining medical images. This process is repeated multiple times to verify the generalization ability of the network. The average of segmentation accuracy is calculated by comparing every segmented image with its gold standard image.},
	booktitle = {2010 10th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Othman, A. A. and Tizhoosh, H. R.},
	month = nov,
	year = {2010},
	note = {ISSN: 2164-7151},
	keywords = {Accuracy, Artificial neural networks, Biomedical imaging, Feature extraction, Image segmentation, Pixel, Training, feature extraction, image analysis, image segmentation, image thresholding, intelligent thresholding, medical image processing, medical images, neural nets, neural network},
	pages = {1159--1164},
}

@inproceedings{li_marker-based_2020-1,
	title = {Marker-{Based} {Shape} {Estimation} of a {Continuum} {Manipulator} {Using} {Binocular} {Vision} and {Its} {Error} {Compensation}},
	doi = {10.1109/ICMA49215.2020.9233616},
	abstract = {Robot-assisted laparoscopy endoscopic single-site surgery robot (LESS) procedures typically involve highly flexible and continuum manipulators to get access to the surgical targets through a small keyhole incision and then perform complicated operations within the abdomen or chest cavity. However, the inherent deformable design and inevitable collisions with the anatomy cause both active and passive deformation of the distal continuum manipulator, which challenges the accurate shape estimation in both structured and unstructured environment. Therefore, it is essential to achieve real-time and accurate and shape tracking of the continuum manipulator for precise and reliable motion control. A new method is proposed in this paper for solving in the problem of real-time shape sensing for the distal continuous robot using binoculars by extracting position information of each markers. However, to cope with the problems of visual occlusion and limited algorithm stability, an error compensation method based on machine learning algorithm is realized. The performance of the proposed algorithm was verified by experiments, and the accuracy of the reconstruction after the correction by the learning algorithm was improved by about 70\%.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	author = {Li, J. and Sun, Y. and Su, H. and Zhang, G. and Shi, C.},
	month = oct,
	year = {2020},
	note = {ISSN: 2152-744X},
	keywords = {Binocular vision, Continuum manipulator, LESS/SPL, MIS, Machine learning, Shape estimation, abdomen, binocular vision, binoculars, deformation, distal continuous robot, distal continuum manipulator, endoscopes, error compensation, error compensation method, inevitable collisions, inherent deformable design, learning (artificial intelligence), machine learning algorithm, manipulators, marker-based shape estimation, medical image processing, medical robotics, motion control, passive deformation, position control, precise motion control, real-time shape sensing, reliable motion control, robot-assisted laparoscopy endoscopic single-site surgery robot procedures, shape estimation, shape tracking, structured environment, surgery, surgical targets},
	pages = {1745--1750},
}

@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2021-02-23},
	journal = {arXiv:1912.01703 [cs, stat]},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01703},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}

@inproceedings{nair_rectified_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Rectified linear units improve restricted boltzmann machines},
	isbn = {978-1-60558-907-7},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	urldate = {2021-02-23},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	month = jun,
	year = {2010},
	pages = {807--814},
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	language = {en},
	urldate = {2021-02-23},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{abidi_intrinsic_2017,
	title = {On {Intrinsic} {Safety} of {Soft} {Robots}},
	volume = {4},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2017.00005/full#h7},
	doi = {10.3389/frobt.2017.00005},
	abstract = {The rapidly growing field of soft robotics owes its success to the vast vistas of possibilities they promise. They may be utilized as standalone systems, or work in harmony with the existing robotic technologies. Being based on soft and/or flexible materials, soft robots have usually high dexterity and at the same time, they are also often considered ‘intrinsically safe’. This is generally true and soft-bodied robots can be considered safer from a mechanical point of view, but this is sometimes improperly used. The identification of possible safety loopholes in soft robots is the subject of this paper. After a general overview of safety in robotics, we reported an overview of the main sources of unsafe conditions that may arise by the use of soft robotics technologies. Safety aspects are discussed in three categories quasi-static, dynamic and material failure. Some safety factors exclusive to soft robots such as whiplash like effect and energy stored in highly strained elements are also introduced. Measures to avoid such unsafe conditions are presented such as establishing operational limits and introduction of inspection regimes and arrest systems.},
	language = {English},
	urldate = {2021-02-20},
	journal = {Frontiers in Robotics and AI},
	author = {Abidi, Haider and Cianchetti, Matteo},
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {Physical human-robot interaction, Safety, Soft actuators, smart materials, soft robotics},
}

@article{hofer_vision-based_2020,
	title = {A {Vision}-based {Sensing} {Approach} for a {Spherical} {Soft} {Robotic} {Arm}},
	url = {http://arxiv.org/abs/2012.06413},
	abstract = {Sensory feedback is essential for the control of soft robotic systems and to enable deployment in a variety of different tasks. Proprioception refers to sensing the robot's own state and is of crucial importance in order to deploy soft robotic systems outside of laboratory environments, i.e. where no external sensing, such as motion capture systems, is available. A vision-based sensing approach for a soft robotic arm made from fabric is presented, leveraging the high-resolution sensory feedback provided by cameras. No mechanical interaction between the sensor and the soft structure is required and consequently, the compliance of the soft system is preserved. The integration of a camera into an inflatable, fabric-based bellow actuator is discussed. Three actuators, each featuring an integrated camera, are used to control the spherical robotic arm and simultaneously provide sensory feedback of the two rotational degrees of freedom. A convolutional neural network architecture predicts the two angles describing the robot's orientation from the camera images. Ground truth data is provided by a motion capture system during the training phase of the supervised learning approach and its evaluation thereafter. The camera-based sensing approach is able to provide estimates of the orientation in real-time with an accuracy of about one degree. The reliability of the sensing approach is demonstrated by using the sensory feedback to control the orientation of the robotic arm in closed-loop.},
	urldate = {2021-02-20},
	journal = {arXiv:2012.06413 [cs]},
	author = {Hofer, Matthias and Sferrazza, Carmelo and D'Andrea, Raffaello},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.06413},
	keywords = {Computer Science - Robotics},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv: 1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
	language = {en},
	urldate = {2021-01-31},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kendall_posenet_2016,
	title = {{PoseNet}: {A} {Convolutional} {Network} for {Real}-{Time} 6-{DOF} {Camera} {Relocalization}},
	shorttitle = {{PoseNet}},
	url = {http://arxiv.org/abs/1505.07427},
	abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
	urldate = {2021-01-31},
	journal = {arXiv:1505.07427 [cs]},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	month = feb,
	year = {2016},
	note = {arXiv: 1505.07427},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{mahendran_3d_2017,
	title = {{3D} {Pose} {Regression} using {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1708.05628},
	abstract = {3D pose estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. Most state-of-the-art approaches to 3D pose estimation solve this problem as a pose-classification problem in which the pose space is discretized into bins and a CNN classifier is used to predict a pose bin. We argue that the 3D pose space is continuous and propose to solve the pose estimation problem in a CNN regression framework with a suitable representation, data augmentation and loss function that captures the geometry of the pose space. Experiments on PASCAL3D+ show that the proposed 3D pose regression approach achieves competitive performance compared to the state-of-the-art.},
	urldate = {2021-01-31},
	journal = {arXiv:1708.05628 [cs]},
	author = {Mahendran, Siddharth and Ali, Haider and Vidal, Rene},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{homberg_robust_2019,
	title = {Robust proprioceptive grasping with a soft robot hand},
	volume = {43},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-018-9754-1},
	doi = {10.1007/s10514-018-9754-1},
	abstract = {This work presents a soft hand capable of robustly grasping and identifying objects based on internal state measurements along with a combined system which autonomously performs grasps. A highly compliant soft hand allows for intrinsic robustness to grasping uncertainties; the addition of internal sensing allows the configuration of the hand and object to be detected. The finger module includes resistive force sensors on the fingertips for contact detection and resistive bend sensors for measuring the curvature profile of the finger. The curvature sensors can be used to estimate the contact geometry and thus to distinguish between a set of grasped objects. With one data point from each finger, the object grasped by the hand can be identified. A clustering algorithm to find the correspondence for each grasped object is presented for both enveloping grasps and pinch grasps. A closed loop system uses a camera to detect approximate object locations. Compliance in the soft hand handles that uncertainty in addition to geometric uncertainty in the shape of the object.},
	language = {en},
	number = {3},
	urldate = {2021-01-30},
	journal = {Autonomous Robots},
	author = {Homberg, Bianca S. and Katzschmann, Robert K. and Dogar, Mehmet R. and Rus, Daniela},
	month = mar,
	year = {2019},
	pages = {681--696},
}

@inproceedings{yamanaka_development_2020,
	title = {Development of a {Food} {Handling} {Soft} {Robot} {Hand} {Considering} a {High}-speed {Pick}-and-place {Task}},
	url = {https://ieeexplore.ieee.org/document/9026282},
	doi = {10.1109/SII46433.2020.9026282},
	abstract = {In recent years, industrial robots have been applied to food production because of its huge potential market. Although cooking a large number of foods is done by specially designed cooking machines, dishing up the cooked foods remains a labor-intensive task. To solve this problem, we developed a new food handling robot hand consisting of simple-shaped soft actuators that can handle easily damaged natural food and convey it at high speed. In this paper, the characteristic of the developed soft actuator was shown and the bending mechanism was analyzed by the finite element method. In addition, the prototyped robot hand was evaluated by grasping and conveying experiments with some food samples.},
	booktitle = {2020 {IEEE}/{SICE} {International} {Symposium} on {System} {Integration} ({SII})},
	author = {Yamanaka, Y. and Katagiri, S. and Nabae, H. and Suzumori, K. and Endo, G.},
	month = jan,
	year = {2020},
	note = {ISSN: 2474-2325},
	keywords = {Actuators, Electron tubes, Production, Ribs, Shape, Soft robotics, cooked foods, developed soft actuator, dexterous manipulators, finite element analysis, food handling robot hand, food handling soft robot hand, food processing industry, food production, food products, food samples, high-speed pick-and-place task, industrial manipulators, industrial robots, natural food, prototyped robot hand, simple-shaped soft actuators, specially designed cooking machines},
	pages = {87--92},
}

@article{wang_dual-mode_2020,
	title = {A dual-mode soft gripper for food packaging},
	volume = {125},
	issn = {0921-8890},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889019300879},
	doi = {10.1016/j.robot.2020.103427},
	abstract = {Robotics and automation in the food industry is not as widely applied as in other industries, such as automotive and electrical industries, due to the large variations in the shape and properties of food materials and the frequent alterations of food products. Robotic end effectors that can adapt to these variations and handle multiple types of food materials are in high demand. Therefore, we propose a dual-mode soft gripper made of rubber material that can grasp and suck different types of objects. The gripper consists of four soft fingers, fabricated with rubber material using casting process, each of which is designed as a combination of a PenuNet bending actuator and a suction pad located at the fingertip. We introduce a new design for the air paths, which play an important role in the proper function of the soft finger. Finite element (FE) simulations were performed to confirm the finger design. Experimental tests were conducted to evaluate single finger bending, gripper lifting force, and grasping and sucking actions for various types of food materials. Results show that the soft gripper can lift a 273.97-g hot dog in the grasping mode, as well as a 512.62-g bagged Kernel corn and a 1072.65-g Macbook Air in the suction mode. It can adapt to approximately circular and square targets, such as a piece of fried chicken and an orange, when the soft fingers are in a perpendicular configuration. While in a parallel configuration, the gripper can successfully handle elongated targets, such as a hot dog. An experiment is also presented to demonstrate the automatic packaging of a Japanese boxed lunch, which requires both grasping and suction modes to be employed.},
	language = {en},
	urldate = {2021-01-29},
	journal = {Robotics and Autonomous Systems},
	author = {Wang, Zhongkui and Or, Keung and Hirai, Shinichi},
	month = mar,
	year = {2020},
	keywords = {Automation, Food packaging, Grasping, Soft gripper, Suction},
	pages = {103427},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2021-01-15},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@article{petrovic_hysteresis_1989,
	title = {Hysteresis effects of minimum fluidization velocity in a draft tube airlift reactor},
	volume = {44},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/000925098985273X},
	doi = {10.1016/0009-2509(89)85273-X},
	language = {en},
	number = {4},
	urldate = {2021-01-06},
	journal = {Chemical Engineering Science},
	author = {Petrović, D. and Pošarac, D. and Skala, D.},
	month = jan,
	year = {1989},
	pages = {996--998},
}

@article{tsinontides_mechanics_1993,
	title = {The mechanics of gas fluidized beds with an interval of stable fluidization},
	volume = {255},
	issn = {1469-7645, 0022-1120},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/mechanics-of-gas-fluidized-beds-with-an-interval-of-stable-fluidization/5476812F89F9C457EEC17A066F869CCB},
	doi = {10.1017/S0022112093002472},
	abstract = {When small, light solid particles are fluidized by gases it is well known that stable expansion occurs over a finite interval of gas flow beyond the point of minimum fluidization. The existence of such an interval can be predicted from linear stability theory provided the momentum equation for the particles contains a sufficiently large term representing an effective pressure that increases with the concentration of the particles. There is at present some controversy regarding the physical origin of such a term. Some workers attribute it to forces exerted between particles at points of solid–solid contact, while others invoke hydrodynamic mechanisms related to the interaction between the particles and the fluid. In this paper the processes of fluidization and defluidization for fine particles are followed very carefully round complete cycles, starting from zero gas flow and extending to a value at which bubbles appear, then back to zero. The depth of the bed and the pressure drop in the gas traversing it are recorded at each stage, and vertical profiles of the volume fraction of particulate material are determined with a high-resolution gamma-ray densitometer. Similar information is also obtained for sub-cycles extending over more restricted intervals of the gas flow rate. The particles studied are cracking catalyst, with mean diameter 75 μm, and Ottawa sand with mean diameter 154 μm. The results lead to the conclusion that the particle assemblies exhibit yield stresses throughout the range of stable behaviour, and thus are not truly fluidized beds, in the accepted sense. The phenomena observed are such that it is most unlikely that their origin is hydrodynamic. For the particular systems studied we therefore conclude that contact forces are responsible for stabilization.},
	language = {en},
	urldate = {2021-01-06},
	journal = {Journal of Fluid Mechanics},
	author = {Tsinontides, S. C. and Jackson, R.},
	month = oct,
	year = {1993},
	note = {Publisher: Cambridge University Press},
	pages = {237--274},
}

@article{loezos_role_2002,
	title = {The role of contact stresses and wall friction on fluidization},
	volume = {57},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/S0009250902004219},
	doi = {10.1016/S0009-2509(02)00421-9},
	abstract = {Fluidization and defluidization experiments, where we increased the gas superficial velocity in small increments and then decreased it, were performed in tubes of different diameters to probe the role of wall friction on pressure drop and bed height. Such experiments, covering the regimes of packed bed, stable bed expansion and bubbling bed, were carried out for several different particles. The compressive yield strength of the particle assemblies at various volume fractions was determined by measuring the height of fully defluidized beds at various mass loading levels. The systematic effect of the tube diameter on pressure drop and bed height hysteresis could be rationalized in terms of a one-dimensional model that accounted for the effect of wall friction and path-dependent contact stresses in the particle phase. Bubbling seemed to set in when the yield stress in the particle assembly could be overcome by the inherent fluctuations. Our experiments, which focused primarily on gas velocities below the minimum bubbling conditions, did not reveal any dramatic change across the Geldart A–B boundary. This is consistent with the original observation by Geldart (Powder Technol. 7 (1973) 285). The distinct difference between beds of group A and B particles in the gently bubbling regime reported by Cody et al. (Powder Technol. 87 (1996) 211) is thus likely to be due to changes in the dynamics of the bubbles, as we observed no striking difference between these beds at gas velocities below minimum bubbling conditions.},
	language = {en},
	number = {24},
	urldate = {2021-01-06},
	journal = {Chemical Engineering Science},
	author = {Loezos, Peter N. and Costamagna, Paola and Sundaresan, Sankaran},
	month = dec,
	year = {2002},
	keywords = {Contact stress, Fluidization, Granular materials, Hydrodynamics, Powders, Wall friction},
	pages = {5123--5141},
}

@article{zhu_gas_2005,
	title = {Gas fluidization characteristics of nanoparticle agglomerates},
	volume = {51},
	copyright = {Copyright © 2005 American Institute of Chemical Engineers (AIChE)},
	issn = {1547-5905},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.10319},
	doi = {https://doi.org/10.1002/aic.10319},
	abstract = {An experimental study is conducted to determine the effect of different types of nanoparticles on the gas fluidization characteristics of nanoparticle agglomerates. Taking advantage of the extremely high porosity of the bed, optical techniques are used to visualize the flow behavior, as well as to measure the sizes of the fluidized nanoparticle agglomerates at the bed surface. Upon fluidizing 11 different nanoparticle materials, two types of nanoparticle fluidization behavior, agglomerate particulate fluidization (APF) and agglomerate bubbling fluidization (ABF), are observed and systematically investigated. A simple analytical model is developed to predict the agglomerate sizes for APF nanoparticles, and the results agree fairly well with the optical measurements. Using the Ergun equation, the experimentally measured pressure drop and bed height, and the average agglomerate size and voidage at minimum fluidization predicted by the model, the minimum fluidization velocities for APF nanoparticles are calculated and also agree well with the experimental values. Other important fluidization features such as bed expansion, bed pressure drop, and hysteresis effects, and the effects of the primary particle size and material properties are also described. © 2005 American Institute of Chemical Engineers AIChE J, 51: 426–439, 2005},
	language = {en},
	number = {2},
	urldate = {2021-01-06},
	journal = {AIChE Journal},
	author = {Zhu, Chao and Yu, Qun and Dave, Rajesh N. and Pfeffer, Robert},
	year = {2005},
	note = {\_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.10319},
	keywords = {agglomerates, bed expansion, fluidization, nanoparticles, pressure drop},
	pages = {426--439},
}

@inproceedings{pal_preprocessing_2016,
	title = {Preprocessing for image classification by convolutional neural networks},
	doi = {10.1109/RTEICT.2016.7808140},
	abstract = {In recent times, the Convolutional Neural Networks have become the most powerful method for image classification. Various researchers have shown the importance of network architecture in achieving better performances by making changes in different layers of the network. Some have shown the importance of the neuron's activation by using various types of activation functions. But here we have shown the importance of preprocessing techniques for image classification using the CIFAR10 dataset and three variations of the Convolutional Neural Network. The results that we have achieved, clearly shows that the Zero Component Analysis(ZCA) outperforms both the Mean Normalization and Standardization techniques for all the three networks and thus it is the most important preprocessing technique for image classification with Convolutional Neural Networks.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Recent} {Trends} in {Electronics}, {Information} {Communication} {Technology} ({RTEICT})},
	author = {Pal, K. K. and Sudeep, K. S.},
	month = may,
	year = {2016},
	keywords = {Biological neural networks, CIFAR10, CIFAR10 dataset, Computer architecture, Conferences, Convolution, Convolutional Neural Network, Convolutional codes, Normalization, Preprocessing, Standardization, Training, ZCA, activation functions, convolutional neural networks, image classification, image classification preprocessing, mean normalization, network architecture, neural nets, neuron activation, standardization techniques, zero component analysis},
	pages = {1778--1781},
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations ar...},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1058--1066},
}

@article{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	urldate = {2020-12-25},
	journal = {arXiv:1411.1792 [cs]},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zamir_taskonomy_2018,
	title = {Taskonomy: {Disentangling} {Task} {Transfer} {Learning}},
	shorttitle = {Taskonomy},
	url = {http://arxiv.org/abs/1804.08328},
	abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
	urldate = {2020-12-25},
	journal = {arXiv:1804.08328 [cs]},
	author = {Zamir, Amir and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08328},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{shao_transfer_2015,
	title = {Transfer {Learning} for {Visual} {Categorization}: {A} {Survey}},
	volume = {26},
	issn = {2162-2388},
	shorttitle = {Transfer {Learning} for {Visual} {Categorization}},
	doi = {10.1109/TNNLS.2014.2330900},
	abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
	number = {5},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Shao, L. and Zhu, F. and Li, X.},
	month = may,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Action recognition, Adaptation models, Algorithms, Humans, Knowledge, Knowledge transfer, Learning systems, Machine Learning, Models, Theoretical, Neural Networks (Computer), Surveys and Questionnaires, Testing, Training, Training data, Transfer (Psychology), Visual Perception, Visualization, human action recognition, image classification, learning (artificial intelligence), machine learning, object recognition, survey, transfer learning, transfer learning algorithms, visual categorization, visual categorization.},
	pages = {1019--1034},
}

@article{weiss_survey_2016,
	title = {A survey of transfer learning},
	volume = {3},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-016-0043-6},
	doi = {10.1186/s40537-016-0043-6},
	abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
	number = {1},
	urldate = {2020-12-25},
	journal = {Journal of Big Data},
	author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
	month = may,
	year = {2016},
	keywords = {Data mining, Domain adaptation, Machine learning, Survey, Transfer learning},
	pages = {9},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2020-12-25},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2020-12-25},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
}

@inproceedings{tang_deep_2010,
	title = {Deep networks for robust visual recognition},
	abstract = {Deep Belief Networks (DBNs) are hierarchi- cal generative models which have been used successfully to model high dimensional visual data. However, they are not robust to com- mon variations such as occlusion and random noise. We explore two strategies for improv- ing the robustness of DBNs. First, we show that a DBN with sparse connections in the rst layer is more robust to variations that are not in the training set. Second, we de- velop a probabilistic denoising algorithm to determine a subset of the hidden layer nodes to unclamp. We show that this can be ap- plied to any feedforward network classier with localized rst layer connections. Recog- nition results after denoising are signicantly better over the standard DBN implementa- tions for various sources of noise.},
	author = {Tang, Yichuan and Eliasmith, Chris},
	month = aug,
	year = {2010},
	pages = {1055--1062},
}

@article{tang_deep_nodate,
	title = {Deep networks for robust visual recognition},
	abstract = {Deep Belief Networks (DBNs) are hierarchical generative models which have been used successfully to model high dimensional visual data. However, they are not robust to common variations such as occlusion and random noise. We explore two strategies for improving the robustness of DBNs. First, we show that a DBN with sparse connections in the ﬁrst layer is more robust to variations that are not in the training set. Second, we develop a probabilistic denoising algorithm to determine a subset of the hidden layer nodes to unclamp. We show that this can be applied to any feedforward network classiﬁer with localized ﬁrst layer connections. Recognition results after denoising are signiﬁcantly better over the standard DBN implementations for various sources of noise.},
	language = {en},
	author = {Tang, Yichuan and Eliasmith, Chris},
	pages = {8},
}

@article{da_costa_empirical_2016,
	title = {An empirical study on the effects of different types of noise in image classification tasks},
	url = {http://arxiv.org/abs/1609.02781},
	abstract = {Image classiﬁcation is one of the main research problems in computer vision and machine learning. Since in most real-world image classiﬁcation applications there is no control over how the images are captured, it is necessary to consider the possibility that these images might be affected by noise (e.g. sensor noise in a low-quality surveillance camera). In this paper we analyse the impact of three different types of noise on descriptors extracted by two widely used feature extraction methods (LBP and HOG) and how denoising the images can help to mitigate this problem. We carry out experiments on two different datasets and consider several types of noise, noise levels, and denoising methods. Our results show that noise can hinder classiﬁcation performance considerably and make classes harder to separate. Although denoising methods were not able to reach the same performance of the noise-free scenario, they improved classiﬁcation results for noisy data.},
	language = {en},
	urldate = {2020-11-30},
	journal = {arXiv:1609.02781 [cs]},
	author = {da Costa, Gabriel B. Paranhos and Contato, Welinton A. and Nazare, Tiago S. and Neto, João E. S. Batista and Ponti, Moacir},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.02781},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{nazare_deep_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Convolutional} {Neural} {Networks} and {Noisy} {Images}},
	isbn = {978-3-319-75193-1},
	doi = {10.1007/978-3-319-75193-1_50},
	abstract = {The presence of noise represent a relevant issue in image feature extraction and classification. In deep learning, representation is learned directly from the data and, therefore, the classification model is influenced by the quality of the input. However, the ability of deep convolutional neural networks to deal with images that have a different quality when compare to those used to train the network is still to be fully understood. In this paper, we evaluate the generalization of models learned by different networks using noisy images. Our results show that noise cause the classification problem to become harder. However, when image quality is prone to variations after deployment, it might be advantageous to employ models learned using noisy data.},
	language = {en},
	booktitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Nazaré, Tiago S. and da Costa, Gabriel B. Paranhos and Contato, Welinton A. and Ponti, Moacir},
	editor = {Mendoza, Marcelo and Velastín, Sergio},
	year = {2018},
	keywords = {Dataset Version, Deep Convolutional Neural Networks, Noise Resilience, Pepper Noise, SVHN Dataset},
	pages = {416--424},
}

@article{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2020-11-27},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-11-27},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kloss_models_2012,
	title = {Models, algorithms and validation for opensource {DEM} and {CFD}–{DEM}},
	volume = {12},
	issn = {1468-4349},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/PCFD.2012.047457},
	doi = {10.1504/PCFD.2012.047457},
	abstract = {We present a multi–purpose CFD–DEM framework to simulate coupled fluid–granular systems. The motion of the particles is resolved by means of the Discrete Element Method (DEM), and the Computational Fluid Dynamics (CFD) method is used to calculate the interstitial fluid flow. We first give a short overview over the DEM and CFD–DEM codes and implementations, followed by elaborating on the numerical schemes and implementation of the CFD–DEM coupling approach, which comprises two fundamentally different approaches, the unresolved CFD–DEM and the resolved CFD–DEM using an Immersed Boundary (IB) method. Both the DEM and the CFD–DEM approach are successfully tested against analytics as well as experimental data.},
	number = {2-3},
	urldate = {2020-11-17},
	journal = {Progress in Computational Fluid Dynamics, an International Journal},
	author = {Kloss, Christoph and Goniva, Christoph and Hager, Alice and Amberger, Stefan and Pirker, Stefan},
	month = jan,
	year = {2012},
	note = {Publisher: Inderscience Publishers},
	pages = {140--152},
}

@book{radl_state_2015,
	title = {State of the art in mapping schemes for dilute and dense {Euler}-{Lagrange} simulations},
	copyright = {© 2015 SINTEF Academic Press},
	isbn = {978-82-536-1432-8},
	url = {https://sintef.brage.unit.no/sintef-xmlui/handle/11250/2464769},
	abstract = {Euler-Lagrange (EL) simulations are an extremely important tool for academia and industry to better understand gas-particle flows. We present simulation results for various gas-particle flow configurations using a variety of Lagrangian-to-Euler coupling schemes. Specifically, we have combined the idea of smoothing the exchange fields (as proposed by Pirker et al. (2011), as well as Capecelatro and Desjardins (2013)) to design a new generation of robust mapping schemes that allow implicit, explicit or a hybrid implicit/explicit time marching. Our schemes enable EL simulations of highly loaded gas-particle flows in which particles have a broad size distribution. We demonstrate the performance of our mapping schemes for the case of (i) a bubbling bi-disperse fluidized bed, (ii) a freely sedimenting suspension, as well as (iii) particle injection in turbulent cross flow configurations.},
	language = {eng},
	urldate = {2020-11-17},
	publisher = {SINTEF Academic Press},
	author = {Radl, Stefan and Gonzales, Begona C. and Goniva, Christoph and Pirker, Stefan},
	year = {2015},
	note = {Accepted: 2017-11-08T07:51:02Z
ISSN: 2387-4295},
}

@article{weller_tensorial_1998,
	title = {A tensorial approach to computational continuum mechanics using object-oriented techniques},
	volume = {12},
	issn = {0894-1866},
	url = {https://aip.scitation.org/doi/abs/10.1063/1.168744},
	doi = {10.1063/1.168744},
	number = {6},
	urldate = {2020-11-17},
	journal = {Computers in Physics},
	author = {Weller, H. G. and Tabor, G. and Jasak, H. and Fureby, C.},
	month = nov,
	year = {1998},
	note = {Publisher: American Institute of Physics},
	pages = {620--631},
}

@article{bird_transport_1961,
	title = {Transport {Phenomena}},
	volume = {108},
	issn = {1945-7111},
	url = {https://iopscience.iop.org/article/10.1149/1.2428074/meta},
	doi = {10.1149/1.2428074},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {Journal of The Electrochemical Society},
	author = {Bird, R. B. and Stewart, W. E. and Lightfoot, E. N. and Meredith, Robert E.},
	month = mar,
	year = {1961},
	note = {Publisher: IOP Publishing},
	pages = {78C},
}

@article{zhou_discrete_2010,
	title = {Discrete particle simulation of particle–fluid flow: model formulations and their applicability},
	volume = {661},
	issn = {0022-1120, 1469-7645},
	shorttitle = {Discrete particle simulation of particle–fluid flow},
	url = {https://www.cambridge.org/core/product/identifier/S002211201000306X/type/journal_article},
	doi = {10.1017/S002211201000306X},
	abstract = {The approach of combining computational fluid dynamics (CFD) for continuum fluid and the discrete element method (DEM) for discrete particles has been increasingly used to study the fundamentals of coupled particle–fluid flows. Different CFD–DEM models have been used. However, the origin and the applicability of these models are not clearly understood. In this paper, the origin of different model formulations is discussed first. It shows that, in connection with the continuum approach, three sets of formulations exist in the CFD–DEM approach: an original format set I, and subsequent derivations of set II and set III, respectively, corresponding to the so-called model A and model B in the literature. A comparison and the applicability of the three models are assessed theoretically and then verified from the study of three representative particle–fluid flow systems: fluidization, pneumatic conveying and hydrocyclones. It is demonstrated that sets I and II are essentially the same, with small differences resulting from different mathematical or numerical treatments of a few terms in the original equation. Set III is however a simplified version of set I. The testing cases show that all the three models are applicable to gas fluidization and, to a large extent, pneumatic conveying. However, the application of set III is conditional, as demonstrated in the case of hydrocyclones. Strictly speaking, set III is only valid when fluid flow is steady and uniform. Set II and, in particular, set I, which is somehow forgotten in the literature, are recommended for the future CFD–DEM modelling of complex particle–fluid flow.},
	language = {en},
	urldate = {2020-11-17},
	journal = {Journal of Fluid Mechanics},
	author = {Zhou, Z. Y. and Kuang, S. B. and Chu, K. W. and Yu, A. B.},
	month = oct,
	year = {2010},
	pages = {482--510},
}

@article{anderson_fluid_1967,
	title = {Fluid {Mechanical} {Description} of {Fluidized} {Beds}. {Equations} of {Motion}},
	volume = {6},
	issn = {0196-4313, 1541-4833},
	url = {https://pubs.acs.org/doi/abs/10.1021/i160024a007},
	doi = {10.1021/i160024a007},
	language = {en},
	number = {4},
	urldate = {2020-11-17},
	journal = {Industrial \& Engineering Chemistry Fundamentals},
	author = {Anderson, T. B. and Jackson, Roy},
	month = nov,
	year = {1967},
	pages = {527--539},
}

@article{goniva_influence_2012,
	title = {Influence of rolling friction on single spout fluidized bed simulation},
	volume = {10},
	issn = {1674-2001},
	url = {http://www.sciencedirect.com/science/article/pii/S1674200112001356},
	doi = {10.1016/j.partic.2012.05.002},
	abstract = {In this paper we study the effect of rolling friction on the dynamics in a single spout fluidized bed using Discrete Element Method (DEM) coupled to Computational Fluid Dynamics (CFD). In a first step we neglect rolling friction and show that the results delivered by the open source CFD–DEM framework applied in this study agree with previous simulations documented in literature. In a second step we include a rolling friction sub-model in order to investigate the effect of particle non-sphericity. The influence of particle–particle as well as particle–wall rolling friction on the flow in single spout fluidized bed is studied separately. Adequate rolling friction model parameters are obtained using first principle DEM simulations and data from literature. Finally, we demonstrate the importance of correct modelling of rolling friction for coupled CFD–DEM simulations of spout fluidized beds. We show that simulation results can be improved significantly when applying a rolling friction model, and that experimental data from literature obtained with Positron Emission Particle Tracking (PEPT) technique can be satisfactorily reproduced.},
	language = {en},
	number = {5},
	urldate = {2020-11-17},
	journal = {Particuology},
	author = {Goniva, Christoph and Kloss, Christoph and Deen, Niels G. and Kuipers, Johannes A. M. and Pirker, Stefan},
	month = oct,
	year = {2012},
	keywords = {CFD–DEM, Non-sphericity, Open source, Rolling friction, Spout fluidized bed},
	pages = {582--591},
}

@article{van_buijtenen_numerical_2011,
	title = {Numerical and experimental study on multiple-spout fluidized beds},
	volume = {66},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/S0009250911001564},
	doi = {10.1016/j.ces.2011.02.055},
	abstract = {In this paper we study the effect of multiple spouts on the bed dynamics in a pseudo-2D triple-spout fluidized bed, employing the discrete particle model (DPM) and non-intrusive measurement techniques such as particle image velocimetry (PIV) and positron emission particle tracking (PEPT). A flow regime map was constructed, revealing new regimes that were not reported so far. The multiple-interacting-spouts regime (C) has been studied in detail for a double- and triple-spout fluidized bed, where the corresponding fluidization regime for a single-spout fluidized bed has been studied as a reference case. The experimental results obtained with PIV and PEPT agree very well for all the three cases, showing the good performance of these techniques. The DPM simulation results slightly deviate from the experiments which is attributed to particle–wall effects that are more dominant in pseudo-2D beds than in 3D systems. The investigated multiple-interacting-spouts regime is a fully new flow regime that does not appear in single-spout fluidized beds. Two flow patterns have been observed, viz. particle circulation in between the spouts near the bottom of the bed, and an apparent single-spout fluidization motion at a higher location upwards in the bed. These findings show that the presence of multiple spouts in a spout fluidized bed highly affect the flow behaviour, which cannot be distinguished by solely investigating single-spout fluidized beds.},
	language = {en},
	number = {11},
	urldate = {2020-11-17},
	journal = {Chemical Engineering Science},
	author = {van Buijtenen, Maureen S. and van Dijk, Willem-Jan and Deen, Niels G. and Kuipers, J. A. M. and Leadbeater, T. and Parker, D. J.},
	month = jun,
	year = {2011},
	keywords = {Discrete particle model, Granulation, Multiple spouts (nozzles), Particle image velocimetry, Positron emission particle tracking, Spout fluidized beds},
	pages = {2368--2376},
}

@article{koch_inertial_2001,
	title = {Inertial {Effects} in {Suspension} and {Porous}-{Media} {Flows}},
	volume = {33},
	url = {https://doi.org/10.1146/annurev.fluid.33.1.619},
	doi = {10.1146/annurev.fluid.33.1.619},
	abstract = {The current understanding of the average flow properties of packed beds and particle suspensions, in which inertia plays a significant role on the particle length scale, is examined. The features of inertial suspensions posing challenges to theoriticians include the nonlinear and unsteady nature of the governing equations, the inability to superimpose solutions, the prevalence of hydrodynamic instabilities, and the existence of particle-particle collisions. We discuss two special cases of inertial suspensions, for which detailed kinetic theories have been developed: (a) particles in a gas, and (b) spherical, high-Reynolds number bubbles in liquid. Subsequently, we review recent applications of computational fluid dynamics to simulate the motion in particle suspensions with both inertia and vorticity in the continueous phase. The synthesis of these analytical and numerical techniques is a promising approach to address the many challenges of modelling inertial suspensions.},
	number = {1},
	urldate = {2020-11-17},
	journal = {Annual Review of Fluid Mechanics},
	author = {Koch, Donald L and Hill, Reghan J},
	year = {2001},
	note = {\_eprint: https://doi.org/10.1146/annurev.fluid.33.1.619},
	pages = {619--647},
}

@article{tsuji_lagrangian_1992,
	title = {Lagrangian numerical simulation of plug flow of cohesionless particles in a horizontal pipe},
	volume = {71},
	issn = {0032-5910},
	url = {http://www.sciencedirect.com/science/article/pii/003259109288030L},
	doi = {10.1016/0032-5910(92)88030-L},
	abstract = {Lagrangian-type numerical simulation was carried out on plug flow of cohesionless, spherical particles conveyed in a horizontal pipe. The motion of individual particles contacting each other was calculated using the equations of motion and a modified Cundall model. Forces between particles were expressed by using the Hertzian contact theory. The Ergun Equation was applied to give the fluid force acting on particles in a moving or stationary bed. The flow patterns obtained in the present work appear to be realistic. The wave-like motion of the flow boundary reported previously by several other researchers was observed clearly in the simulation. Also, good agreement was obtained for the relation between the height of the stationary deposited layer and the plug flow velocity. Due to the limitations of computation time, only the case of large particles (ie. d {\textgreater} 10 mm) could be considered here.},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {Powder Technology},
	author = {Tsuji, Y. and Tanaka, T. and Ishida, T.},
	month = sep,
	year = {1992},
	pages = {239--250},
}

@article{cundall_discrete_1979,
	title = {A discrete numerical model for granular assemblies},
	volume = {29},
	issn = {0016-8505},
	url = {https://www.icevirtuallibrary.com/doi/abs/10.1680/geot.1979.29.1.47},
	doi = {10.1680/geot.1979.29.1.47},
	abstract = {The distinct element method is a numerical model capable of describing the mechanical behaviour of assemblies of discs and spheres. The method is based on the use of an explicit numerical scheme in which the interaction of the particles is monitored contact by contact and the motion of the particles modelled particle by particle. The main features of the distinct element method are described. The method is validated by comparing force vector plots obtained from the computer program BALL with the corresponding plots obtained from a photoelastic analysis. The photoelastic analysis used for the comparison is the one applied to an assembly of discs by De Josselin de Jong and Verruijt (1969). The force vector diagrams obtained numerically closely resemble those obtained photoelastically. It is concluded from this comparison that the distinct element method and the program BALL are valid tools for research into the behaviour of granular assemblies. La méthode des éléments distincts est un modèle numérique capable de décrire le comportement mécanique de l'assemblage de disques et de sphères. La méthode est basée sur l'utilisation d'un système numérique explicite dans lequel l'interaction des particules est contrôlée contact par contact et le mouvement des particules simulé particule par particule. Les caracteristiques principales de la méthode des eléments distints sont décrites. La méthode est validée en comparant les tracés de vecteur de force obtenus par le programme sur ordicateur BALL avec les tracés correspondants obtanus a l'aide d'une analyse photo-élastique. L'analyse photo-élastique utilisée pour la comparaison est celle appliquée sur un assemblage de disques par De Josselin de Jong et Verruijt (1969). Les diagrammes de vecteur de force obtenus numériquement sont très voisins de ceux obtenus photo-élastiquement. Cette comparaison permet de conclure que la methode des éléments distincts et le programme BALL sont des instruments valables pour la recherche du comportement des assemblages granulaires.},
	number = {1},
	urldate = {2020-11-17},
	journal = {Géotechnique},
	author = {Cundall, P. A. and Strack, O. D. L.},
	month = mar,
	year = {1979},
	note = {Publisher: ICE Publishing},
	pages = {47--65},
}

@book{norouzi_coupled_2016,
	title = {Coupled {CFD}-{DEM} {Modeling}: {Formulation}, {Implementation} and {Application} to {Multiphase} {Flows}},
	isbn = {978-1-119-00529-2},
	shorttitle = {Coupled {CFD}-{DEM} {Modeling}},
	abstract = {Discusses the CFD-DEM method of modeling which combines both the Discrete Element Method and Computational Fluid Dynamics to simulate fluid-particle interactions.  Deals with both theoretical and practical concepts of CFD-DEM, its numerical implementation accompanied by a hands-on numerical code in FORTRAN Gives examples of industrial applications},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Norouzi, Hamid Reza and Zarghami, Reza and Sotudeh-Gharebagh, Rahmat and Mostoufi, Navid},
	month = oct,
	year = {2016},
	note = {Google-Books-ID: lQlPDQAAQBAJ},
	keywords = {Science / Chemistry / General, Technology \& Engineering / Chemical \& Biochemical},
}

@article{deen_review_2007,
	series = {Fluidized {Bed} {Applications}},
	title = {Review of discrete particle modeling of fluidized beds},
	volume = {62},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/S0009250906004830},
	doi = {10.1016/j.ces.2006.08.014},
	abstract = {This paper reviews the use of discrete particle models (DPMs) for the study of the flow phenomena prevailing in fluidized beds. DPMs describe the gas-phase as a continuum, whereas each of the individual particles is treated as a discrete entity. The DPMs accounts for the gas–particle and particle–particle interactions. This model is part of a multi-level modeling approach and has proven to be very useful to generate closure information required in more coarse-grained models. In this paper, a basic DPM, based on both the hard- and soft-sphere approaches is described. The importance of the closures for particle–particle and gas–particle interaction is demonstrated with several illustrative examples. Finally, an outlook for the use of DPMs for the investigation of various chemical engineering problems in the area of fluidization is given.},
	language = {en},
	number = {1},
	urldate = {2020-11-17},
	journal = {Chemical Engineering Science},
	author = {Deen, N. G. and Van Sint Annaland, M. and Van der Hoef, M. A. and Kuipers, J. A. M.},
	month = jan,
	year = {2007},
	keywords = {Discrete particle model, Fluidization, Multi-scale modeling},
	pages = {28--44},
}

@book{noauthor_coupled_nodate,
	title = {Coupled {CFD}-{DEM} {Modeling}},
	url = {https://books.google.com/books/about/Coupled_CFD_DEM_Modeling.html?id=7DQWDQAAQBAJ},
	abstract = {Discusses the CFD-DEM method of modeling which combines both the Discrete Element Method and Computational Fluid Dynamics to simulate fluid-particle interactions. Deals with both theoretical and practical concepts of CFD-DEM, its numerical implementation accompanied by a hands-on numerical code in FORTRAN Gives examples of industrial applications},
	urldate = {2020-11-17},
}

@article{zhu_discrete_2007,
	series = {Frontier of {Chemical} {Engineering} - {Multi}-scale {Bridge} between {Reductionism} and {Holism}},
	title = {Discrete particle simulation of particulate systems: {Theoretical} developments},
	volume = {62},
	issn = {0009-2509},
	shorttitle = {Discrete particle simulation of particulate systems},
	url = {http://www.sciencedirect.com/science/article/pii/S000925090700262X},
	doi = {10.1016/j.ces.2006.12.089},
	abstract = {Particle science and technology is a rapidly developing interdisciplinary research area with its core being the understanding of the relationships between micro- and macroscopic properties of particulate/granular matter—a state of matter that is widely encountered but poorly understood. The macroscopic behaviour of particulate matter is controlled by the interactions between individual particles as well as interactions with surrounding fluids. Understanding the microscopic mechanisms in terms of these interaction forces is therefore key to leading to truly interdisciplinary research into particulate matter and producing results that can be generally used. This aim can be effectively achieved via particle scale research based on detailed microdynamic information such as the forces acting on and trajectories of individual particles in a considered system. In recent years, such research has been rapidly developed worldwide, mainly as a result of the rapid development of discrete particle simulation technique and computer technology. This paper reviews the work in this area with special reference to the discrete element method and associated theoretical developments. It covers three important aspects: models for the calculation of the particle–particle and particle–fluid interaction forces, coupling of discrete element method with computational fluid dynamics to describe particle–fluid flow, and the theories for linking discrete to continuum modelling. Needs for future development are also discussed.},
	language = {en},
	number = {13},
	urldate = {2020-11-17},
	journal = {Chemical Engineering Science},
	author = {Zhu, H. P. and Zhou, Z. Y. and Yang, R. Y. and Yu, A. B.},
	month = jul,
	year = {2007},
	keywords = {Mathematical modelling, Multiphase flow, Particulate processes, Powder technology, Simulation},
	pages = {3378--3396},
}

@article{sveen_introduction_2004,
	title = {An introduction to {MatPIV} v. 1.6.1},
	url = {https://www.duo.uio.no/handle/10852/10196},
	abstract = {Particle Image Velocimetry (PIV) has seen a rapid growth over the last two decades, much owing to the developments in digital cameras and solid state laser technologies. PIV can essentially be looked upon as an application of pattern mathching principles to experiments. We rely upon hardware such as lasers and cameras for illumination and image capture. Subsequently we use computer code to perform the pattern matching. {\textless}B{\textgreater}MatPIV{\textless}/B{\textgreater} is one of a variety of different computer coedes available written specifically for this purpose. The vast majority of codes are commercially available, but in the last 7-8 years several Open Source PIV codes have been created and are currently being more or less actively maintained. MatPIV is one of these codes and it is distributed under the GNU General Public License [{\textless}B{\textgreater}MatPIV{\textless}/B{\textgreater} is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. The GNU General Public License can be found on the World Wide Web at http://www.gnu.org/copyleft/gpl.html or it can be obtained by writing to the Free Software Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.] This document acts as the entry level tutorial fr using the MatPIV code. Very basic theory is reviewed and references to appropiate sources are included. The focus, however, is on the use of {\textless}B{\textgreater}MatPIV{\textless}/B{\textgreater} and how it is implemented.},
	language = {eng},
	urldate = {2020-11-17},
	author = {Sveen, Johan Kristian},
	year = {2004},
	note = {Accepted: 2013-03-12T08:18:23Z
Publisher: Matematisk Institutt, Universitetet i Oslo},
}

@book{j_davidson_fluidised_nodate,
	title = {Fluidised {Particles}},
	author = {{J. Davidson} and Harrison, D.},
}

@article{pigford_fluidized_1968,
	title = {Fluidized {Particles}. {By} {J}. {F}. {DAVIDSON} and {D}. {HARRISON}. {Cambridge} {University} {Press}, 1963. 155 pp. 35s. or \$6.50.},
	volume = {33},
	issn = {1469-7645, 0022-1120},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/fluidized-particles-by-j-f-davidson-and-d-harrison-cambridge-university-press-1963-155-pp-35s-or-650/049958D2AA18C35D16B29E8476C5C3CF},
	doi = {10.1017/S0022112068221560},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0022112068221560/resource/name/firstPage-S0022112068001564a.jpg},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {Journal of Fluid Mechanics},
	author = {Pigford, R. L.},
	month = sep,
	year = {1968},
	note = {Publisher: Cambridge University Press},
	pages = {623--624},
}

@article{machu_coalescence_2001,
	title = {Coalescence, torus formation and breakup of sedimenting drops: experiments and computer simulations},
	volume = {447},
	issn = {0022-1120, 1469-7645},
	shorttitle = {Coalescence, torus formation and breakup of sedimenting drops},
	url = {https://www.cambridge.org/core/product/identifier/S0022112001005882/type/journal_article},
	doi = {10.1017/S0022112001005882},
	abstract = {The motion and shape evolution of viscous drops made from a dilute suspension of 
tiny, spherical glass beads sedimenting in an otherwise quiescent liquid is investigated 
both experimentally and theoretically for conditions of low Reynolds number. In 
the (presumed) absence of any significant interfacial tension, the Bond number 
[Bscr    ] = (Δρ)
              gR
              2
              /σ is effectively infinite. 
The key stages of deformation of single drops and 
pairs of interacting drops are identified. Of particular interest are (i) the coalescence of 
two trailing drops, (ii) the subsequent formation of a torus, and (iii) the breakup of the 
torus into two or more droplets in a repeating cascade. To overcome limitations of the 
boundary-integral method in tracking highly deformed interfaces and coalescing and 
dividing drops, we develop a formal analogy between drops of homogeneous liquid 
and a dilute, uniformly distributed swarm of sedimenting particles, for which only 
the 1/
              r
              far-field hydrodynamic interactions are important. Simple, robust numerical 
simulations using only swarms of Stokeslets reproduce the main phenomena observed 
in the classical experiments and in our flow-visualization studies. Detailed particle 
image velocimetry (PIV) for axisymmetric configurations enable a mechanistic analysis 
and confirm the theoretical results. We expose the crucial importance of the initial 
condition – why a single spherical drop does not deform substantially, but a pair 
of spherical drops, or a bell-shaped drop similar to what is actually formed in the 
laboratory, does undergo the torus/breakup transformation. The extreme sensitivity 
of the streamlines to the shape of the ring-like swarm explains why the ring that 
initially forms in the experiments does not behave like the slender open torus analysed 
asymptotically by Kojima, Hinch \& Acrivos (1984). Essentially all of the phenomena 
described above can be explained within the realm of Stokes flow, without resort to 
interfacial tension or inertial effects.},
	language = {en},
	urldate = {2020-11-17},
	journal = {Journal of Fluid Mechanics},
	author = {Machu, Gunther and Meile, Walter and Nitsche, Ludwig C. and Schaflinger, Uwe},
	month = nov,
	year = {2001},
	pages = {299--336},
}

@article{thomson_v_1886,
	title = {V. {On} the formation of vortex rings by drops falling into liquids, and some allied phenomena},
	volume = {39},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspl.1885.0034},
	doi = {10.1098/rspl.1885.0034},
	abstract = {When a drop of ink falls into water from not too great a height, it descends through the water as a ring, in which there is evidently considerable rotation about the circular axis passing through the centres of its cross sections; as the ring travels down through the water inequalities make their appearance : more ink seems to collect in some parts of it than in others, and as these parts of the ring descend more rapidly than the rest, it assumes some such appearance as that shown in fig. 1.},
	number = {239-241},
	urldate = {2020-11-17},
	journal = {Proceedings of the Royal Society of London},
	author = {Thomson, Joseph John and Newall, Hugh Frank},
	month = jan,
	year = {1886},
	note = {Publisher: Royal Society},
	pages = {417--436},
}

@article{adachi_behavior_1978,
	title = {The behavior of a swarm of particles moving in a viscous fluid},
	volume = {33},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/0009250978850775},
	doi = {10.1016/0009-2509(78)85077-5},
	abstract = {A significant bulk flow is thought to occur if a part of the body of a suspension, which has a different density from the density of the remainder of the suspension, is large enough in space however slight the difference may be. As a typical case, the slow motion of a swarm of particles in a viscous fluid was studied both theoretically and experimentally. Under a certain condition, the suspended particles formed a drop of suspension, in which the suspended particles and fluid moved as one body in the same way. The falling velocity of a spherical swarm of particles, which was called a drop of suspension, was measured and compared with a new theoretical prediction presented in this paper. It may be also noted that the theoretical model may be applicable to the flow due to density difference by means of replacement of the continuous mass by the damped mass which is regarded as consisting of particles.},
	language = {en},
	number = {1},
	urldate = {2020-11-17},
	journal = {Chemical Engineering Science},
	author = {Adachi, K. and Kiriyama, S. and Yoshioka, N.},
	month = jan,
	year = {1978},
	pages = {115--121},
}

@article{nitsche_break-up_1997,
	title = {Break-up of a falling drop containing dispersed particles},
	volume = {340},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112097005223/type/journal_article},
	doi = {10.1017/S0022112097005223},
	abstract = {The general purpose of this paper is to investigate some consequences
 

of the randomness of the velocities of interacting rigid particles falling
 under 

gravity through viscous 

fluid at small Reynolds number. Random velocities often imply diffusive
 transport 

of the particles, but particle diffusion of the conventional kind exists
 only 

when the 

length characteristic of the diffusion process is small compared with the
 distance 

over which the particle concentration is effectively uniform. When this
 condition 

is not satisfied, some alternative analytical description of the dispersion
 

process is 

needed. Here we suppose that a dilute dispersion of sedimenting particles
 is bounded 

externally by pure fluid and enquire about the rate at which particles
 make outward 

random crossings of the (imaginary) boundary. If the particles are initially
 

distributed 

with uniform concentration within a spherical boundary, we gain the convenience
 

of approximately steady conditions with a velocity distribution like that
 in 

a falling 

spherical drop of pure liquid. However, randomness of the particle velocities
 causes 

some particles to make an outward crossing of the spherical boundary and
 to be 

carried round the boundary and thence downstream in a vertical ‘tail’.
 

This is the nature of break-up of a falling cloud of particles.
            A numerical simulation of the motion of a number of interacting particles
 

(maximum 320) assumed to act as Stokeslets confirms the validity of the
 above picture 

of the way in which particles leak away from a spherical cluster of particles.
 A 

dimensionally correct empirical relation for the rate at which particles
 are 

lost from 

the cluster involves a constant which is indeed found to depend only weakly
 on the 

various parameters occurring in the numerical simulation. According to
 this relation 

the rate at which particles are lost from the blob is proportional to the
 

fall speed of 

an isolated particle and to the area of the blob boundary. Some photographs
 of a 

leaking tail of particles in figure 5 also provide support for the 

qualitative picture.},
	language = {en},
	urldate = {2020-11-17},
	journal = {Journal of Fluid Mechanics},
	author = {Nitsche, J. M. and Batchelor, G. K.},
	month = jun,
	year = {1997},
	pages = {161--175},
}

@article{epstein_liquid_1985,
	title = {Liquid fluidization of binary particle mixtures—{II}. {Bed} inversion},
	volume = {40},
	issn = {0009-2509},
	url = {http://www.sciencedirect.com/science/article/pii/0009250985800932},
	doi = {10.1016/0009-2509(85)80093-2},
	abstract = {When the size ratio and the density ratio of the two species in a binary solids mixture are on opposite sides of unity, it is possible in some cases for such mixtures to display an inversion phenomenon during liquid fluidization whereby the order in which the two species are vertically segregated is reversed on changing the liquid velocity. This inversion phenomenon, originally reported by Hancock and recently revived in the literature, is investigated experimentally and modelled theoretically. The necessary and sufficient conditions for a bed inversion to occur are derived. Reasonable agreement between theory and experiment is demonstrated. The effect of particle size spread is examined, and the model is also used to rationalize long standing mineral dressing rules on sorting and sizing of particles. Finally some practical implications of the inversion phenomenon are designated.},
	language = {en},
	number = {8},
	urldate = {2020-11-17},
	journal = {Chemical Engineering Science},
	author = {Epstein, N. and LeClair, B. P.},
	month = jan,
	year = {1985},
	pages = {1517--1526},
}

@article{mclaren_gravitational_2019,
	title = {Gravitational instabilities in binary granular materials},
	volume = {116},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/19/9263},
	doi = {10.1073/pnas.1820820116},
	abstract = {The motion and mixing of granular media are observed in several contexts in nature, often displaying striking similarities to liquids. Granular dynamics occur in geological phenomena and also enable technologies ranging from pharmaceuticals production to carbon capture. Here, we report the discovery of a family of gravitational instabilities in granular particle mixtures subject to vertical vibration and upward gas flow, including a Rayleigh–Taylor (RT)-like instability in which lighter grains rise through heavier grains in the form of “fingers” and “granular bubbles.” We demonstrate that this RT-like instability arises due to a competition between upward drag force increased locally by gas channeling and downward contact forces, and thus the physical mechanism is entirely different from that found in liquids. This gas channeling mechanism also generates other gravitational instabilities: the rise of a granular bubble which leaves a trail of particles behind it and the cascading branching of a descending granular droplet. These instabilities suggest opportunities for patterning within granular mixtures.},
	language = {en},
	number = {19},
	urldate = {2020-11-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {McLaren, Christopher P. and Kovar, Thomas M. and Penn, Alexander and Müller, Christoph R. and Boyce, Christopher M.},
	month = may,
	year = {2019},
	pmid = {31010930},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {fluidization, granular material, instabilities},
	pages = {9263--9268},
}

@article{vinningland_granular_2007,
	title = {Granular {Rayleigh}-{Taylor} {Instability}: {Experiments} and {Simulations}},
	volume = {99},
	shorttitle = {Granular {Rayleigh}-{Taylor} {Instability}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.048001},
	doi = {10.1103/PhysRevLett.99.048001},
	abstract = {A granular instability driven by gravity is studied experimentally and numerically. The instability arises as grains fall in a closed Hele-Shaw cell where a layer of dense granular material is positioned above a layer of air. The initially flat front defined by the grains subsequently develops into a pattern of falling granular fingers separated by rising bubbles of air. A transient coarsening of the front is observed right from the start by a finger merging process. The coarsening is later stabilized by new fingers growing from the center of the rising bubbles. The structures are quantified by means of Fourier analysis and quantitative agreement between experiment and computation is shown. This analysis also reveals scale invariance of the flow structures under overall change of spatial scale.},
	number = {4},
	urldate = {2020-11-17},
	journal = {Physical Review Letters},
	author = {Vinningland, Jan Ludvig and Johnsen, Øistein and Flekkøy, Eirik G. and Toussaint, Renaud and Måløy, Knut Jørgen},
	month = jul,
	year = {2007},
	note = {Publisher: American Physical Society},
	pages = {048001},
}

@article{lange_fingering_1998,
	title = {Fingering instability in a water-sand mixture},
	volume = {4},
	issn = {1434-6036},
	url = {https://doi.org/10.1007/s100510050405},
	doi = {10.1007/s100510050405},
	abstract = {The temporal evolution of a water-sand interface driven by gravity is experimentally investigated. By means of a Fourier analysis of the evolving interface the growth rates are determined for the different modes appearing in the developing front. To model the observed behavior we apply the idea of the Rayleigh-Taylor instability for two stratified fluids. Carrying out a linear stability analysis we calculate the growth rates from the corresponding dispersion relations for finite and infinite cell sizes. Based on the theoretical results the viscosity of the suspension is estimated to be approximately 100 times higher than that of pure water, in agreement with other experimental findings.},
	language = {en},
	number = {4},
	urldate = {2020-11-17},
	journal = {The European Physical Journal B - Condensed Matter and Complex Systems},
	author = {Lange, A. and Schröter, M. and Scherer, M.A. and Engel, A. and Rehberg, I.},
	month = aug,
	year = {1998},
	pages = {475--484},
}

@article{taylor_instability_1950,
	title = {The instability of liquid surfaces when accelerated in a direction perpendicular to their planes. {I}},
	volume = {201},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1950.0052},
	doi = {10.1098/rspa.1950.0052},
	abstract = {It is shown that, when two superposed fluids of different densities are accelerated in a direction perpendicular to their interface, this surface is stable or unstable according to whether the acceleration is directed from the heavier to the lighter fluid or vice versa. The relationship between the rate of development of the instability and the length of wave-like disturbances, the acceleration and the densities is found, and similar calculations are made for the case when a sheet of liquid of uniform depth is accelerated.},
	number = {1065},
	urldate = {2020-11-17},
	journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
	author = {Taylor, Geoffrey Ingram},
	month = mar,
	year = {1950},
	note = {Publisher: Royal Society},
	pages = {192--196},
}

@article{abanades_capture_2004,
	title = {Capture of {CO2} from combustion gases in a fluidized bed of {CaO}},
	volume = {50},
	copyright = {Copyright © 2004 American Institute of Chemical Engineers (AIChE)},
	issn = {1547-5905},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.10132},
	doi = {https://doi.org/10.1002/aic.10132},
	abstract = {Experiments in a pilot-scale fluidized-bed reactor have been carried out to investigate the carbonation reaction of CaO, as a potential method for CO2 capture from combustion flue gases at high-temperatures. Results show that CO2 capture efficiencies are very high, while there is a sufficient fraction of CaO in the bed reacting in the fast reaction regime. The total capture capacity of the bed decays with the number of carbonation-calcination cycles. The experimental CO2 concentration profiles measured inside the bed during the fast reaction period are interpreted with the KL fluid bed model, by supplying information on sorbent deactivation from laboratory tests. It is concluded that a fluidized bed of CaO can be a suitable reactor to achieve very effective CO2 capture efficiencies from a combustion flue gas. © 2004 American Institute of Chemical Engineers AIChE J, 50:1614–1622, 2004},
	language = {en},
	number = {7},
	urldate = {2020-11-17},
	journal = {AIChE Journal},
	author = {Abanades, J. Carlos and Anthony, Edward J. and Lu, Dennis Y. and Salvador, Carlos and Alvarez, Diego},
	year = {2004},
	note = {\_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.10132},
	keywords = {CO2 capture, carbonation, fluidized beds, global warming, modeling},
	pages = {1614--1622},
}

@article{muzzio_powder_2002,
	title = {Powder technology in the pharmaceutical industry: the need to catch up fast},
	volume = {124},
	issn = {0032-5910},
	shorttitle = {Powder technology in the pharmaceutical industry},
	url = {http://www.sciencedirect.com/science/article/pii/S003259100100482X},
	doi = {10.1016/S0032-5910(01)00482-X},
	abstract = {Pharmaceutical product development and manufacturing, which is largely an exercise in particle technology, is in serious need of technical upgrading. In this article, an overview of the current state of the art is provided, along with a discussion of expected research trends and their economic and societal impacts. In particular, the anticipated role of nanotechnology is discussed in some detail.},
	language = {en},
	number = {1},
	urldate = {2020-11-17},
	journal = {Powder Technology},
	author = {Muzzio, Fernando J and Shinbrot, Troy and Glasser, Benjamin J},
	month = apr,
	year = {2002},
	keywords = {Nanotechnology, Pharmaceutical industry, Powder technology},
	pages = {1--7},
}

@article{daerr_two_1999,
	title = {Two types of avalanche behaviour in granular media},
	volume = {399},
	copyright = {1999 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/20392},
	doi = {10.1038/20392},
	abstract = {The nature of the transition between static and flowing regimes in granular media1,2 provides a key to understanding their dynamics. When a pile of sand starts flowing, avalanches occur on its inclined free surface. Previously, studies3 of avalanches in granular media have considered the time series of avalanches in rotating drums4, or in piles continuously fed with material. Here we investigate single avalanches created by perturbing a static layer of glass beads on a rough inclined plane. We observe two distinct types of avalanche, with evidence for different underlying physical mechanisms. Perturbing a thin layer results in an avalanche propagating downhill and also laterally owing to collisions between neighbouring grains, causing triangular tracks; perturbing a thick layer results in an avalanche front that also propagates upwards, grains located uphill progressively tumbling down because of loss of support. The perturbation threshold for triggering an avalanche is found to decrease to zero at a critical slope. Our results may improve understanding of naturally occurring avalanches on snow slopes5 where triangular tracks are also observed.},
	language = {en},
	number = {6733},
	urldate = {2020-11-17},
	journal = {Nature},
	author = {Daerr, Adrian and Douady, Stéphane},
	month = may,
	year = {1999},
	note = {Number: 6733
Publisher: Nature Publishing Group},
	pages = {241--243},
}

@article{bagnold_movement_1936,
	title = {The movement of desert sand},
	volume = {157},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1936.0218},
	doi = {10.1098/rspa.1936.0218},
	abstract = {It is well known that on a dry sand beach and, on a much larger scale, on sand-strewn desert country the wind, if above a certain strength, will cause the surface sand grains to rise and to travel down-wind as a low-flying cloud. The mechanism, however, by which (a) the grains composing this cloud are raised, (b) the rate of mass movement of the sand depends upon the wind velocity, or (c) the wind velocity close to the surface is affected by the presence of the sand cloud, does not appear to have been previously investigated experimentally. This mutual interaction of wind and sand grains is of interest both in connexion with the problem of the tendency of sand to heap itself up into dunes even in totally flat uniform plains, and also for the light it may throw on certain aspects of the allied problem of the transport of sediment by liquid currents. Sand found in the desert is usually composed of rounded quartz grains whose sizes range from small pebbles 2 to 3 mm. in diameter down to small particles 0∙01 mm. in diameter, which must be regarded as dust. Mechanical analysis of eolian sand for grain size show, when curves of percentage weight are plotted against grain size, that the peaks of such curves never occur on the small side of 0∙15 mm. diameter. Sand having this smallest peak size is found at the crests of dunes. Here the grains approach uniformity of size, so that the diagrams are sharp-peaked. On the other hand, sand deposits clear of the actual dunes give broad, low diagrams with the peak at a larger diameter. In every case the diagrams show only a few per cent by weight at a size of 0∙03 mm. In fact, it is a peculiarity of all sand accumulations that they are practically free from dust.},
	number = {892},
	urldate = {2020-11-17},
	journal = {Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences},
	author = {Bagnold, Ralph Alger and Taylor, Geoffrey Ingram},
	month = dec,
	year = {1936},
	note = {Publisher: Royal Society},
	pages = {594--620},
}

@article{della_santina_improved_2020,
	title = {On an {Improved} {State} {Parametrization} for {Soft} {Robots} {With} {Piecewise} {Constant} {Curvature} and {Its} {Use} in {Model} {Based} {Control}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2967269},
	abstract = {Piecewise constant curvature models have proven to be an useful tool for describing kinematics and dynamics of soft robots. However, in their three dimensional formulation they suffer from many issues limiting their range of applicability - as discontinuities and singularities - mainly concerning the straight configuration of the robot. In this work we analyze these flaws, and we show that they are not due to the piecewise constant curvature assumption itself, but that instead they are a byproduct of the commonly employed direction/angle of bending parametrization of the state. We therefore consider an alternative state representation which solves all the discussed issues, and we derive a model based controller based on it. Examples in simulation are provided to support and describe the theoretical results. When using the novel parametrization, the system is able to perform more complex tasks, with a strongly reduced computational burden, and without incurring in spikes and discontinuous behaviors.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Della Santina, Cosimo and Bicchi, A. and Rus, D.},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Manifolds, Modeling, Robot kinematics, Silicon, Soft robotics, Standards, alternative state representation, and learning for soft robots, bending, control, dimensional formulation, improved state parametrization, kinematics, model based control, model based controller, motion control, natural machine motion, nonlinear control systems, novel parametrization, piecewise constant curvature assumption, piecewise constant curvature models, piecewise constant techniques, robot kinematics, soft robots, state-space methods, straight configuration},
	pages = {1001--1008},
}

@article{tome_lifting_2017,
	title = {Lifting from the {Deep}: {Convolutional} {3D} {Pose} {Estimation} from a {Single} {Image}},
	shorttitle = {Lifting from the {Deep}},
	url = {http://arxiv.org/abs/1701.00295},
	doi = {10.1109/CVPR.2017.603},
	abstract = {We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state- of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.},
	urldate = {2020-10-19},
	journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Tome, Denis and Russell, Chris and Agapito, Lourdes},
	month = jul,
	year = {2017},
	note = {arXiv: 1701.00295
version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {5689--5698},
}

@inproceedings{toshev_deeppose_2014,
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	shorttitle = {{DeepPose}},
	doi = {10.1109/CVPR.2014.214},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Computational modeling, DNN-based regression problem, DeepPose, Detectors, Estimation, Joints, Measurement, Training, Vectors, academic benchmarks, body joints, cascades, deep learning, deep neural networks, human pose estimation, neural nets, neural networks, pose estimation, real-world images, regression analysis},
	pages = {1653--1660},
}

@inproceedings{kocabas_self-supervised_2019,
	title = {Self-{Supervised} {Learning} of {3D} {Human} {Pose} {Using} {Multi}-{View} {Geometry}},
	doi = {10.1109/CVPR.2019.00117},
	abstract = {Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/EpipolarPose.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {2D ground-truth poses, 3D from Single Image, 3D ground-truth data, 3D human pose estimators, EpipolarPose, Face, Gesture, Human3.6M dataset, MPI-INF-3DHP dataset, and Body Pose, camera geometry, camera parameters, cameras, computational geometry, epipolar geometry, learning (artificial intelligence), multiview geometry, multiview images, performance measure, pose estimation, pose estimation methods, pose structure score, self-supervised learning method},
	pages = {1077--1086},
}

@article{choi_learning_2018,
	title = {Learning {Object} {Grasping} for {Soft} {Robot} {Hands}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2810544},
	abstract = {We present a three-dimensional deep convolutional neural network (3D CNN) approach for grasping unknown objects with soft hands. Soft hands are compliant and capable of handling uncertainty in sensing and actuation, but come at the cost of unpredictable deformation of the soft fingers. Traditional model-driven grasping approaches, which assume known models for objects, robot hands, and stable grasps with expected contacts, are inapplicable to such soft hands, since predicting contact points between objects and soft hands is not straightforward. Our solution adopts a deep CNN approach to find good caging grasps for previously unseen objects by learning effective features and a classifier from point cloud data. Unlike recent CNN models applied to robotic grasping which have been trained on 2D or 2.5D images and limited to a fixed top grasping direction, we exploit the power of a 3D CNN model to estimate suitable grasp poses from multiple grasping directions (top and side directions) and wrist orientations, which has great potential for geometry-related robotic tasks. Our soft hands guided by the 3D CNN algorithm show 87\% successful grasping on previously unseen objects. A set of comparative evaluations shows the robustness of our approach with respect to noise and occlusions.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Choi, Changhyun and Schwarting, Wilko and DelPreto, Joseph and Rus, Daniela},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {3D CNN model, Data models, Grasping, Perception for grasping and manipulation, Robot sensing systems, Solid modeling, Three-dimensional displays, Wrist, deep CNN approach, deep learning in robotics and automation, feedforward neural nets, fixed top grasping direction, good caging grasps, grippers, learning (artificial intelligence), multiple grasping directions, object grasping, recent CNN models, robot vision, robotic grasping, soft fingers, soft robot hands, stable grasps, three-dimensional deep convolutional neural network approach, traditional model-driven grasping approaches, unseen objects},
	pages = {2370--2377},
}

@article{oberweger_hands_2016,
	title = {Hands {Deep} in {Deep} {Learning} for {Hand} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1502.06807},
	abstract = {We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map. We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions. We also show how to use context efficiently to deal with ambiguities between fingers. These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.},
	urldate = {2020-10-18},
	journal = {arXiv:1502.06807 [cs]},
	author = {Oberweger, Markus and Wohlhart, Paul and Lepetit, Vincent},
	month = dec,
	year = {2016},
	note = {arXiv: 1502.06807},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{della_santina_model-based_2020,
	title = {Model-based dynamic feedback control of a planar soft robot: trajectory tracking and interaction with the environment:},
	copyright = {© The Author(s) 2020},
	shorttitle = {Model-based dynamic feedback control of a planar soft robot},
	url = {https://journals.sagepub.com/doi/10.1177/0278364919897292},
	doi = {10.1177/0278364919897292},
	abstract = {Leveraging the elastic bodies of soft robots promises to enable the execution of dynamic motions as well as compliant and safe interaction with an unstructured ...},
	language = {en},
	urldate = {2020-10-16},
	journal = {The International Journal of Robotics Research},
	author = {Della Santina, Cosimo and Katzschmann, Robert K. and Bicchi, Antonio and Rus, Daniela},
	month = jan,
	year = {2020},
	note = {Publisher: SAGE PublicationsSage UK: London, England},
}

@inproceedings{katzschmann_dynamically_2019,
	title = {Dynamically {Closed}-{Loop} {Controlled} {Soft} {Robotic} {Arm} using a {Reduced} {Order} {Finite} {Element} {Model} with {State} {Observer}},
	doi = {10.1109/ROBOSOFT.2019.8722804},
	abstract = {This paper presents a computationally efficient method to model and simulate soft robots. Finite element methods enable us to simulate and control soft robots, but require us to work with a large dimensional system. This limits their use in real-time simulation and makes those methods less suitable for control design tools. Using model order reduction, it is possible to create a reduced order system for building controllers and observers. Model reduction errors are taken into account in the design of the low-order feedback, and it is then applied to the large dimensional, unreduced model. The control architecture is based on a linearized model of the robot and enables the control of the robot around this equilibrium point. To show the performance of this control method, pose-to-pose and trajectory tracking experiments are conducted on a pneumatically actuated soft arm. The soft arm has 12 independent interior cavities that can be pressurized and cause the arm to move in three dimensions. The arm is made of a rubber material and is casted through a lost-wax fabrication technique.},
	booktitle = {2019 2nd {IEEE} {International} {Conference} on {Soft} {Robotics} ({RoboSoft})},
	author = {Katzschmann, Robert K. and Thieffry, Maxime and Goury, Olivier and Kruszewski, Alexandre and Guerra, Thierry-Marie and Duriez, Christian and Rus, Daniela},
	month = apr,
	year = {2019},
	keywords = {Cavity resonators, Computational modeling, Finite element analysis, Manipulators, Mathematical model, Soft robotics, building controllers, closed loop systems, computationally efficient method, control architecture, control design tools, control method, control soft robots, control system synthesis, dexterous manipulators, dimensional model, dimensional system, dynamically closed-loop controlled soft robotic arm, feedback, finite element analysis, finite element methods, linearized model, low-order feedback, model order reduction, model reduction errors, observers, pneumatic actuators, pneumatically actuated soft arm, real-time simulation, reduced order finite element model, reduced order system, reduced order systems, state observer, unreduced model},
	pages = {717--724},
}

@inproceedings{katzschmann_dynamic_2019,
	title = {Dynamic {Motion} {Control} of {Multi}-{Segment} {Soft} {Robots} {Using} {Piecewise} {Constant} {Curvature} {Matched} with an {Augmented} {Rigid} {Body} {Model}},
	doi = {10.1109/ROBOSOFT.2019.8722799},
	abstract = {Despite the emergence of many soft-bodied robotic systems, model-based feedback control for soft robots has remained an open challenge. This is largely due to the intrinsic difficulties in designing controllers for systems with infinite dimensions. This work extends our previously proposed formulation for the dynamics of a soft robot from two to three dimensions. The formulation connects the soft robot's dynamic behavior to a rigid-bodied robot with parallel elastic actuation. The matching between the two systems is exact under the hypothesis of Piecewise Constant Curvature. Based on this connection, we introduce a control architecture with the aim of achieving accurate curvature and bending control. This controller accounts for the natural softness of the system moving in three dimensions, and for the dynamic forces acting on the system. The controller is validated in a realistic simulation, together with a kinematic inversion algorithm. The paper also introduces a soft robot capable of three-dimensional motion, that we use to experimentally validate our control strategy.},
	booktitle = {2019 2nd {IEEE} {International} {Conference} on {Soft} {Robotics} ({RoboSoft})},
	author = {Katzschmann, Robert K. and Santina, Cosimo Della and Toshimitsu, Yasunori and Bicchi, Antonio and Rus, Daniela},
	month = apr,
	year = {2019},
	keywords = {Aerospace electronics, Dynamics, Kinematics, Manipulators, Soft robotics, Three-dimensional displays, accurate curvature, actuators, augmented rigid body model, bending, bending control, control architecture, control strategy, dynamic forces, dynamic motion control, elasticity, feedback, infinite dimensions, kinematic inversion algorithm, mobile robots, model-based feedback control, motion control, multisegment soft robots, natural softness, piecewise constant curvature, position control, rigid-bodied robot, robot dynamics, soft robot capable, soft robot dynamic behavior, soft-bodied robotic systems},
	pages = {454--461},
}

@inproceedings{ye_accurate_2011,
	title = {Accurate {3D} pose estimation from a single depth image},
	doi = {10.1109/ICCV.2011.6126310},
	abstract = {This paper presents a novel system to estimate body pose configuration from a single depth map. It combines both pose detection and pose refinement. The input depth map is matched with a set of pre-captured motion exemplars to generate a body configuration estimation, as well as semantic labeling of the input point cloud. The initial estimation is then refined by directly fitting the body configuration with the observation (e.g., the input depth). In addition to the new system architecture, our other contributions include modifying a point cloud smoothing technique to deal with very noisy input depth maps, a point cloud alignment and pose search algorithm that is view-independent and efficient. Experiments on a public dataset show that our approach achieves significantly higher accuracy than previous state-of-art methods.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Ye, Mao and Xianwang Wang and Yang, Ruigang and Liu Ren and Pollefeys, Marc},
	month = nov,
	year = {2011},
	note = {ISSN: 2380-7504},
	keywords = {3D pose estimation, Accuracy, Cameras, Databases, Estimation, Joints, Sensors, Shape, body pose configuration estimation, human motion modeling, image matching, image motion analysis, input depth map, input point cloud semantic labeling, object detection, point cloud alignment, point cloud smoothing technique, pose detection, pose estimation, pose refinement, pose search algorithm, single depth image, smoothing methods},
	pages = {731--738},
}

@article{tome_lifting_nodate,
	title = {Lifting {From} the {Deep}: {Convolutional} {3D} {Pose} {Estimation} {From} a {Single} {Image}},
	abstract = {We propose a uniﬁed formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to reﬁne the search for better 2D locations. The entire process is trained end-to-end, is extremely efﬁcient and obtains stateof-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.},
	language = {en},
	author = {Tome, Denis and Russell, Chris and Agapito, Lourdes},
	pages = {10},
}

@inproceedings{chen_3d_2017,
	address = {Honolulu, HI},
	title = {{3D} {Human} {Pose} {Estimation} = {2D} {Pose} {Estimation} + {Matching}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100093/},
	doi = {10.1109/CVPR.2017.610},
	abstract = {We explore 3D human pose estimation from a single RGB image. While many approaches try to directly predict 3D pose from image measurements, we explore a simple architecture that reasons through intermediate 2D pose predictions. Our approach is based on two key observations (1) Deep neural nets have revolutionized 2D pose estimation, producing accurate 2D predictions even for poses with self-occlusions (2) ”Big-data”sets of 3D mocap data are now readily available, making it tempting to “lift” predicted 2D poses to 3D through simple memorization (e.g., nearest neighbors). The resulting architecture is straightforward to implement with off-the-shelf 2D pose estimation systems and 3D mocap libraries. Importantly, we demonstrate that such methods outperform almost all state-of-theart 3D pose estimation systems, most of which directly try to regress 3D pose from 2D measurements.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Ching-Hang and Ramanan, Deva},
	month = jul,
	year = {2017},
	pages = {5759--5767},
}

@inproceedings{tompson_efficient_2015,
	title = {Efficient {Object} {Localization} {Using} {Convolutional} {Networks}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Tompson_Efficient_Object_Localization_2015_CVPR_paper.html},
	urldate = {2020-10-15},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	year = {2015},
	pages = {648--656},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Deep learning, Feature extraction, Neural networks, Object detection, Task analysis, Training, convolutional neural nets, convolutional neural network, deep learning-based object detection, face detection, feature extraction, image classification, image features, image understanding, learning (artificial intelligence), network architecture, neural net architecture, neural network, object detection, pedestrian detection, salient object detection, scene classifiers, video analysis},
	pages = {3212--3232},
}

@inproceedings{camarillo_vision_2008,
	title = {Vision based 3-{D} shape sensing of flexible manipulators},
	doi = {10.1109/ROBOT.2008.4543656},
	abstract = {Rigid robotic manipulators employ traditional sensors such as encoders or potentiometers to measure joint angles and determine end-effector position. Manipulators that are flexible, however, introduce motions that are much more difficult to measure. This is especially true for continuum manipulators that articulate by means of material compliance. In this paper, we present a vision based system for quantifying the 3-D shape of a flexible manipulator in real-time. The sensor system is validated for accuracy with known point measurements and for precision by estimating a known 3-D shape. We present two applications of the validated system relating to the open-loop control of a tendon driven continuum manipulator. In the first application, we present a new continuum manipulator model and use the sensor to quantify 3-D performance. In the second application, we use the shape sensor system for model parameter estimation in the absence of tendon tension information.},
	booktitle = {2008 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Camarillo, David B. and Loewke, Kevin E. and Carlson, Christopher R. and Salisbury, J. Kenneth},
	month = may,
	year = {2008},
	note = {ISSN: 1050-4729},
	keywords = {Goniometers, Machine vision, Manipulators, Position measurement, Potentiometers, Real time systems, Robot sensing systems, Sensor systems, Shape measurement, Tendons, continuum manipulators, end effectors, end-effector position, flexible manipulators, image sensors, joint angles, open loop systems, open-loop control, parameter estimation, rigid robotic manipulators, robot vision, shape sensor system, vision based 3D shape sensors, vision based system},
	pages = {2940--2947},
}

@article{barhak_parameterization_2001,
	title = {Parameterization and reconstruction from {3D} scattered points based on neural network and {PDE} techniques},
	volume = {7},
	issn = {1941-0506},
	doi = {10.1109/2945.910817},
	abstract = {Reverse engineering ordinarily uses laser scanners since they can sample 3D data quickly and accurately relative to other systems. These laser scanner systems, however, yield an enormous amount of irregular and scattered digitized point data that requires intensive reconstruction processing. Reconstruction of freeform objects consists of two main stages: parameterization and surface fitting. Selection of an appropriate parameterization is essential for topology reconstruction as well as surface fitness. Current parameterization methods have topological problems that lead to undesired surface fitting results, such as noisy self-intersecting surfaces. Such problems are particularly common with concave shapes whose parametric grid is self-intersecting, resulting in a fitted surface that considerably twists and changes its original shape. In such cases, other parameterization approaches should be used in order to guarantee non-self-intersecting behavior. The parameterization method described in this paper is based on two stages: 2D initial parameterization; and 3D adaptive parameterization. Two methods were developed for the first stage: partial differential equation (PDE) parameterization and neural network self organizing maps (SOM) parameterization. The Gradient Descent Algorithm (GDA) and Random Surface Error Correction (RSEC), both of which are iterative surface fitting methods, were developed and implemented.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Barhak, J. and Fischer, A.},
	month = jan,
	year = {2001},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {2D initial parameterization, 3D adaptive parameterization, 3D scattered points, Gradient Descent Algorithm, Neural networks, Noise shaping, Partial differential equations, Random Surface Error Correction, Reverse engineering, Scattering parameters, Self organizing feature maps, Shape, Surface fitting, Surface reconstruction, Topology, computational geometry, freeform object reconstruction, image reconstruction, iterative methods, neural network, noisy self-intersecting surfaces, partial differential equation parameterization, partial differential equations, self organizing maps, self-organising feature maps, solid modeling, solid modelling, surface fitting, topology reconstruction},
	pages = {1--16},
}

@article{gu_neural_1995,
	title = {Neural network approach to the reconstruction of freeform surfaces for reverse engineering},
	volume = {27},
	issn = {0010-4485},
	url = {http://www.sciencedirect.com/science/article/pii/0010448595907533},
	doi = {10.1016/0010-4485(95)90753-3},
	abstract = {Reconstruction and manufacturing of existing freeform surfaces are of paramount importance for reverse engineering. These are particularly useful for the situations where the surfaces are partially damaged or the surface models are not available. In order to generate models based on the existing freeform surfaces, the surfaces are first digitized by either a laser scanner or a coordinate measuring machine. The digitized points are then used to construct the models. The paper presents a neural network approach to reconstruction of computer models for existing freeform surfaces, and manufacturing of surfaces. To evaluate the effectiveness of the approach, a mathematically known surface, a nonuniform B-spline surface, was used for generating a number of samples for training the networks. Three 4-layered neural networks were designed and trained using a modified back propagation algorithm. The trained networks then generated a number of new points which were compared with the calculated points using the known surface equations. These points were also used to generate toolpaths for machining the surface. The results show that the approach can be used to reconstruct computer representations of the existing surfaces, and to manufacture these surfaces. An example is included to illustrate the approach.},
	language = {en},
	number = {1},
	urldate = {2020-10-15},
	journal = {Computer-Aided Design},
	author = {Gu, P and Yan, X},
	month = jan,
	year = {1995},
	keywords = {freeform surfaces, neural networks, reverse engineering},
	pages = {59--64},
}

@book{faugeras_geometry_2001,
	title = {The {Geometry} of {Multiple} {Images}: {The} {Laws} that {Govern} the {Formation} of {Multiple} {Images} of a {Scene} and {Some} of {Their} {Applications}},
	isbn = {978-0-262-56204-1},
	shorttitle = {The {Geometry} of {Multiple} {Images}},
	abstract = {This book formalizes and analyzes the relations between multiple views of a scene from the perspective of various types of geometries. A key feature is that it considers Euclidean and affine geometries as special cases of projective geometry. Over the last forty years, researchers have made great strides in elucidating the laws of image formation, processing, and understanding by animals, humans, and machines. This book describes the state of knowledge in one subarea of vision, the geometric laws that relate different views of a scene. Geometry, one of the oldest branches of mathematics, is the natural language for describing three-dimensional shapes and spatial relations. Projective geometry, the geometry that best models image formation, provides a unified framework for thinking about many geometric problems are relevant to vision. The book formalizes and analyzes the relations between multiple views of a scene from the perspective of various types of geometries. A key feature is that it considers Euclidean and affine geometries as special cases of projective geometry. Images play a prominent role in computer communications. Producers and users of images, in particular three-dimensional images, require a framework for stating and solving problems. The book offers a number of conceptual tools and theoretical results useful for the design of machine vision algorithms. It also illustrates these tools and results with many examples of real applications.},
	language = {en},
	publisher = {MIT Press},
	author = {Faugeras, Olivier and Luong, Quang-Tuan and Papadopoulo, Théo},
	year = {2001},
	note = {Google-Books-ID: vauYE0nlFGEC},
	keywords = {Mathematics / Geometry / General},
}

@article{kumar_curve_2004,
	title = {Curve and surface reconstruction from points: an approach based on self-organizing maps},
	volume = {5},
	issn = {1568-4946},
	shorttitle = {Curve and surface reconstruction from points},
	url = {http://www.sciencedirect.com/science/article/pii/S1568494604000572},
	doi = {10.1016/j.asoc.2004.04.003},
	abstract = {Modeling of shapes for free form objects from point cloud is an emerging trend. Recognition of shape from the measured point data is a key step in the process of converting discrete data set into a piecewise smooth, continuous model. Shape recognition is to find the topological relation among the points, and in case of thick unorganized point cloud, the step requires both thinning and ordering. The present paper outlines a new approach based on growing self-organizing maps (GSOM) for piecewise linear reconstruction of curves and surfaces from unorganized thick point data. Inferences on selection of self-organizing map (SOM) algorithm parameters for this problem domain have been derived after extensive experimentation. A better quality measure to evaluate and compare various runs of SOM for the domain of curve and surface reconstruction has also been presented.},
	language = {en},
	number = {1},
	urldate = {2020-10-15},
	journal = {Applied Soft Computing},
	author = {Kumar, G. Saravana and Kalra, Prem Kumar and Dhande, Sanjay G.},
	month = dec,
	year = {2004},
	keywords = {Self-organizing maps, Surface reconstruction, Unorganized point cloud},
	pages = {55--66},
}

@inproceedings{berthilsson_reconstruction_1997,
	title = {Reconstruction of {3D}-curves from {2D}-images using affine shape methods for curves},
	doi = {10.1109/CVPR.1997.609368},
	abstract = {In this paper, we propose an algorithm for doing reconstruction of general 3D-curves from a number of 2D-images taken by uncalibrated cameras. No point correspondences between the images are assumed. The curve and the view points are uniquely reconstructed, modulo common projective transformations and the point correspondence problem is solved. Furthermore, the algorithm is independent of the choice of coordinates, as it is based on orthogonal projections and aligning subspaces. The algorithm is based on an extension of affine shape of finite point configurations to more general objects.},
	booktitle = {Proceedings of {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Berthilsson, R. and Astrom, K.},
	month = jun,
	year = {1997},
	note = {ISSN: 1063-6919},
	keywords = {2D images, 3D curves reconstruction, Cameras, Cities and towns, Councils, Eigenvalues and eigenfunctions, Image reconstruction, Reconstruction algorithms, Robustness, Shape, Surface reconstruction, affine shape, affine shape methods, computational geometry, computer vision, finite point configurations, image reconstruction, modulo common projective transformations, orthogonal projections, point correspondence problem, uncalibrated cameras},
	pages = {476--481},
}

@article{camarillo_mechanics_2008,
	title = {Mechanics {Modeling} of {Tendon}-{Driven} {Continuum} {Manipulators}},
	volume = {24},
	issn = {1941-0468},
	doi = {10.1109/TRO.2008.2002311},
	abstract = {Continuum robotic manipulators articulate due to their inherent compliance. Tendon actuation leads to compression of the manipulator, extension of the actuators, and is limited by the practical constraint that tendons cannot support compression. In light of these observations, we present a new linear model for transforming desired beam configuration to tendon displacements and vice versa. We begin from first principles in solid mechanics by analyzing the effects of geometrically nonlinear tendon loads. These loads act both distally at the termination point and proximally along the conduit contact interface. The resulting model simplifies to a linear system including only the bending and axial modes of the manipulator as well as the actuator compliance. The model is then manipulated to form a concise mapping from beam configuration-space parameters to n redundant tendon displacements via the internal loads and strains experienced by the system. We demonstrate the utility of this model by implementing an optimal feasible controller. The controller regulates axial strain to a constant value while guaranteeing positive tendon forces and minimizing their magnitudes over a range of articulations. The mechanics-based model from this study provides insight as well as performance gains for this increasingly ubiquitous class of manipulators.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Camarillo, David B. and Milne, Christopher F. and Carlson, Christopher R. and Zinn, Michael R. and Salisbury, J. Kenneth},
	month = dec,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Cable drive, Capacitive sensors, Force control, Hydraulic actuators, Linear systems, Manipulators, Optimal control, Robots, Solids, Strain control, Tendons, conduit contact interface, elastic manipulator, elasticity, flexible manipulator, kinematics, linear system, manipulator dynamics, manipulator kinematics, mechanics modeling, medical robotics, nonlinear control systems, nonlinear tendon loads, optimal feasible controller, redundant tendons, robotic surgery, slack tendon, snake robot, statics, tendon drive, tendon-driven continuum manipulators},
	pages = {1262--1273},
}

@article{poston_rapid_nodate,
	title = {Rapid {3D} tube reconstruction from nearby views},
	abstract = {We reconstruct a 3D tube (catheter or artery) from nearby X-ray views. Construct 2D skeleton curves from the image edge pro les by wave propagation, thin and smooth them, then reconstruct a 3D curve by epipolar correspondence, resolving side-point singularities by di erential-geometric models. The resulting curve is highly accurate on data from a simulated catheter, whose `true' 3D curve is known. Finding it from edges takes less than a second, su cient for `on the spot' reconstruction, and uses only the moving views normally created by the angiographer, rather than constrained and costly arrangements such as biplanar cameras.},
	language = {en},
	author = {Poston, Tim},
	pages = {11},
}

@inproceedings{webster_design_2005,
	title = {Design {Considerations} for {Robotic} {Needle} {Steering}},
	doi = {10.1109/ROBOT.2005.1570666},
	abstract = {Many medical procedures involve the use of needles, but targeting accuracy can be limited due to obstacles in the needle’s path, shifts in target position caused by tissue deformation, and undesired bending of the needle after insertion. In order to address these limitations, we have developed robotic systems that actively steer a needle in soft tissue. A bevel (asymmetric) tip causes the needle to bend during insertion, and steering is enhanced when the needle is very flexible. An experimental needle steering robot was designed that includes force/torque sensing, horizontal needle insertion, stereo image data acquisition, and controlled actuation of needle rotation and translation. Experiments were performed with a phantom tissue to determine the effects of insertion velocity and bevel tip angle on the needle path, as well as the forces acting on the needle during insertion. Results indicate that needle steering inside tissue does not depend on insertion velocity, but does depend on bevel tip angle. In addition, the forces acting on the needle are directly related to the insertion velocity.},
	booktitle = {Proceedings of the 2005 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Webster, R.J. and Memisevic, J. and Okamura, A.M.},
	month = apr,
	year = {2005},
	note = {ISSN: 1050-4729},
	keywords = {Biomedical imaging, Deformable models, Error correction, Finite element methods, Medical diagnostic imaging, Medical robotics, Medical treatment, Needles, Robot sensing systems, Ultrasonic imaging, medical robotics, needle steering, nonholonomic systems},
	pages = {3588--3594},
}

@article{webster_nonholonomic_2006,
	title = {Nonholonomic {Modeling} of {Needle} {Steering}},
	volume = {25},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364906065388},
	doi = {10.1177/0278364906065388},
	abstract = {As a flexible needle with a bevel tip is pushed through soft tissue, the asymmetry of the tip causes the needle to bend. We propose that, by using nonholonomic kinematics, control, and path planning, an appropriately designed needle can be steered through tissue to reach a specified 3D target. Such steering capability could enhance targeting accuracy and may improve outcomes for percutaneous therapies, facilitate research on therapy effectiveness, and eventually enable new minimally invasive techniques. In this paper, we consider a first step toward active needle steering: design and experimental validation of a nonholonomic model for steering flexible needles with bevel tips. The model generalizes the standard three degree-of-freedom (DOF) nonholonomic unicycle and bicycle models to 6 DOF using Lie group theory. Model parameters are fit using experimental data, acquired via a robotic device designed for the specific purpose of inserting and steering a flexible needle. The experiments quantitatively validate the bevel-tip needle steering model, enabling future research in flexible needle path planning, control, and simulation.},
	language = {en},
	number = {5-6},
	urldate = {2020-10-15},
	journal = {The International Journal of Robotics Research},
	author = {Webster, Robert J. and Kim, Jin Seob and Cowan, Noah J. and Chirikjian, Gregory S. and Okamura, Allison M.},
	month = may,
	year = {2006},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {509--525},
}

@incollection{siciliano_closed-form_2009,
	address = {Berlin, Heidelberg},
	title = {Closed-{Form} {Differential} {Kinematics} for {Concentric}-{Tube} {Continuum} {Robots} with {Application} to {Visual} {Servoing}},
	volume = {54},
	isbn = {978-3-642-00195-6 978-3-642-00196-3},
	url = {http://link.springer.com/10.1007/978-3-642-00196-3_56},
	abstract = {Active cannulas, so named because of their potential medical applications, are a new class of continuum robots consisting of precurved, telescoping, elastic tubes. As individual component tubes are actuated at the base relative to one another, an active cannula changes shape to minimize stored elastic energy. For the ﬁrst time, we derive the diﬀerential kinematics of a general n tube active cannula while accounting for torsional compliance, which can strongly aﬀect the accuracy of robot tip pose prediction. Our derivation makes several explicit assumptions that have never been vetted for a robotic task, so we experimentally validate the Jacobian using a three-link prototype in a simple stereo visual servoing scheme. Our visual servoing experiments validate the Jacobian and also demonstrate the feasibility of using active cannulas under image guidance—a key step toward realizing their potential to reach dexterously through small, winding environments, which is of particular importance in medical applications.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {Experimental {Robotics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Webster, Robet J. and Swensen, John P. and Romano, Joseph M. and Cowan, Noah J.},
	editor = {Siciliano, Bruno and Khatib, Oussama and Groen, Frans and Khatib, Oussama and Kumar, Vijay and Pappas, George J.},
	year = {2009},
	doi = {10.1007/978-3-642-00196-3_56},
	note = {Series Title: Springer Tracts in Advanced Robotics},
	pages = {485--494},
}

@inproceedings{hannan_vision_2003,
	title = {Vision based shape estimation for continuum robots},
	volume = {3},
	doi = {10.1109/ROBOT.2003.1242123},
	abstract = {The investigation of continuum robots has become an area of considerable interest in the last several years. Unlike conventional robotic manipulators which bend in discrete locations, continuum robots bend over continuous sections. One of the main issues that is hampering research in this area is the determination of the robot's shape. In this paper we present a shape-determining scheme that is based on machine vision. The approach uses a high speed camera, an engineered environment, and image processing to determine the shape of our continuum robot called the Elephant's Trunk Manipulator. We present experimental results showing the effectiveness of the technique.},
	booktitle = {2003 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({Cat}. {No}.{03CH37422})},
	author = {Hannan, M. and Walker, I.},
	month = sep,
	year = {2003},
	note = {ISSN: 1050-4729},
	keywords = {Cameras, Charge coupled devices, Charge-coupled image sensors, Image processing, Legged locomotion, Machine vision, Manipulators, Robot sensing systems, Robot vision systems, Shape measurement, cameras, computer vision, continuum robots, curve fitting, elephant trunk manipulator, high speed camera, image processing, machine vision, manipulators, robot shape, robotic manipulators, vision based shape estimation},
	pages = {3449--3454 vol.3},
}

@inproceedings{chitrakaran_setpoint_2004,
	address = {Boston, MA, USA},
	title = {Setpoint regulation of continuum robots using a fixed camera},
	isbn = {978-0-7803-8335-7},
	url = {https://ieeexplore.ieee.org/document/1386789/},
	doi = {10.23919/ACC.2004.1386789},
	abstract = {In this paper, we investigate the problem of rneosuring the shape of D continuum robot using visual information from a fired camWO. Specifically, w e capture the motion of a set of fictitious planes, each formed bg four feature points, defined at various Strategic locolioas along the body o/ the robot. Then, utilizing ezpmssions for the robot forward kinematics a8 well QS the de{\textasciicircum} composition of a homography d a t i n g a refercnce i m a g e of the robot to the a c t e d robot imagc, w e obtain the three dimensional shape information continuously. W e then use this informotion to demonstrate the development of a kinematic controller to reyulete the end-effector of the robot t o (I constant desired position and orieritotion, whzle at thc .same time using its kinematic redundancy to sntisfv D subtask objective such as obstacle avoidance.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {Proceedings of the 2004 {American} {Control} {Conference}},
	publisher = {IEEE},
	author = {Chitrakaran, V.K. and Behal, A. and Dawson, D.M. and Walker, I.D.},
	year = {2004},
	keywords = {continuum robots, kinematic control, motion tracking, shape estimation},
	pages = {1504--1509 vol.2},
}

@inproceedings{croom_visual_2010,
	address = {Anchorage, AK},
	title = {Visual sensing of continuum robot shape using self-organizing maps},
	isbn = {978-1-4244-5038-1},
	url = {http://ieeexplore.ieee.org/document/5509461/},
	doi = {10.1109/ROBOT.2010.5509461},
	abstract = {Shape control of continuum robots requires a means of sensing the the curved shape of the robot. Since continuum robots are deformable, they take on shapes that are general curves in space, which are not fully deﬁned by actuator positions. Vision-based shape-estimation provides a promising avenue for shape-sensing. While this is often facilitated by ﬁducial markers, sometimes ﬁducials are not feasible due to either the robot’s application or its size. To address this, we present a robust and efﬁcient stereo-vision-based, shapesensing algorithm for continuum robots that does not rely on ﬁducials or assume orthogonal camera placement. The algorithm employs self-organizing maps to triangulate threedimensional backbone curves. Experiments with an object with a known shape demonstrate an average accuracy of 1.53 mm on a 239 mm arc length curve.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Croom, Jordan M and Rucker, D Caleb and Romano, Joseph M and Webster, Robert J},
	month = may,
	year = {2010},
	keywords = {computer vision, continuum robots, shape estimation, stereo vision},
	pages = {4591--4596},
}

@article{webster_design_2010,
	title = {Design and {Kinematic} {Modeling} of {Constant} {Curvature} {Continuum} {Robots}: {A} {Review}},
	volume = {29},
	issn = {0278-3649, 1741-3176},
	shorttitle = {Design and {Kinematic} {Modeling} of {Constant} {Curvature} {Continuum} {Robots}},
	url = {http://journals.sagepub.com/doi/10.1177/0278364910368147},
	doi = {10.1177/0278364910368147},
	abstract = {Continuum robotics has rapidly become a rich and diverse area of research, with many designs and applications demonstrated. Despite this diversity in form and purpose, there exists remarkable similarity in the fundamental simplified kinematic models that have been applied to continuum robots. However, this can easily be obscured, especially to a newcomer to the field, by the different applications, coordinate frame choices, and analytical formalisms employed. In this paper we review several modeling approaches in a common frame and notational convention, illustrating that for piecewise constant curvature, they produce identical results. This discussion elucidates what has been articulated in different ways by a number of researchers in the past several years, namely that constant-curvature kinematics can be considered as consisting of two separate submappings: one that is general and applies to all continuum robots, and another that is robot-specific. These mappings are then developed both for the singlesection and for the multi-section case. Similarly, we discuss the decomposition of differential kinematics (the robot’s Jacobian) into robot-specific and robot-independent portions. The paper concludes with a perspective on several of the themes of current research that are shaping the future of continuum robotics.},
	language = {en},
	number = {13},
	urldate = {2020-10-15},
	journal = {The International Journal of Robotics Research},
	author = {Webster, Robert J. and Jones, Bryan A.},
	month = nov,
	year = {2010},
	keywords = {kinematic modeling, piecewise constant curvature},
	pages = {1661--1683},
}

@article{marin-jimenez_3d_2018,
	title = {{3D} human pose estimation from depth maps using a deep combination of poses},
	url = {http://arxiv.org/abs/1807.05389},
	abstract = {Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predeﬁned 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is deﬁned as a ConvNet that computes the speciﬁc weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging ‘ITOP’ and ‘UBC3V’ datasets, which respectively depict realistic and synthetic samples, deﬁning a new state-of-the-art on them.},
	language = {en},
	urldate = {2020-10-15},
	journal = {arXiv:1807.05389 [cs]},
	author = {Marin-Jimenez, Manuel J. and Romero-Ramirez, Francisco J. and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.05389},
	keywords = {3D human pose estimation, computer vision, depth map},
}

@inproceedings{della_santina_dynamic_2018,
	address = {Livorno},
	title = {Dynamic control of soft robots interacting with the environment},
	isbn = {978-1-5386-4516-1},
	url = {https://ieeexplore.ieee.org/document/8404895/},
	doi = {10.1109/ROBOSOFT.2018.8404895},
	abstract = {Despite the emergence of many soft-bodied robotic systems, model-based feedback control has remained an open challenge. This is largely due to the intrinsic difﬁculties in designing controllers for systems with inﬁnite dimensions. In this paper we propose an alternative formulation of the soft robot dynamics which connects the robot’s behavior with the one of a rigid bodied robot with elasticity in the joints. The matching between the two system is exact under the common hypothesis of Piecewise Constant Curvature. Based on this connection we introduce two control architectures, with the aim of achieving accurate curvature control and Cartesian regulation of the robot’s impedance, respectively. The curvature controller accounts for the natural softness of the system, while the Cartesian controller adapts the impedance of the end effector for interactions with an unstructured environment. This work proposes the ﬁrst closed loop dynamic controller for a continuous soft robot. The controllers are validated and evaluated on a physical soft robot capable of planar manipulation.},
	language = {en},
	urldate = {2020-10-12},
	booktitle = {2018 {IEEE} {International} {Conference} on {Soft} {Robotics} ({RoboSoft})},
	publisher = {IEEE},
	author = {Della Santina, Cosimo and Katzschmann, Robert K. and Biechi, Antonio and Rus, Daniela},
	month = apr,
	year = {2018},
	keywords = {2D soft robotic arm, dynamic closed-loop control, motion tracking, piecewise constant curvature},
	pages = {46--53},
}

@article{marchese_whole_nodate,
	title = {Whole {Arm} {Planning} for a {Soft} and {Highly} {Compliant} {2D} {Robotic} {Manipulator}},
	abstract = {Soft continuum manipulators have the advantage of being more compliant and having more degrees of freedom than rigid redundant manipulators. This attribute should allow soft manipulators to autonomously execute highly dexterous tasks. However, current approaches to motion planning, inverse kinematics, and even design limit the capacity of soft manipulators to take full advantage of their inherent compliance. We provide a computational approach to whole arm planning for a soft planar manipulator that advances the arm’s end effector pose in task space while simultaneously considering the arm’s entire envelope in proximity to a conﬁned environment. The algorithm solves a series of constrained optimization problems to determine locally optimal inverse kinematics. Due to inherent limitations in modeling the kinematics of a highly compliant soft robot and the local optimality of the planner’s solutions, we also rely on the increased softness of our newly designed manipulator to accomplish the whole arm task, namely the arm’s ability to harmlessly collide with the environment. We detail the design and fabrication of the new modular manipulator as well as the planner’s central algorithm. We experimentally validate our approach by showing that the robotic system is capable of autonomously advancing the soft arm through a pipe-like environment in order to reach distinct goal states.},
	language = {en},
	author = {Marchese, Andrew and Katzschmann, Robert and Rus, Daniela},
	pages = {7},
}

@article{katzschmann_autonomous_2015,
	title = {Autonomous {Object} {Manipulation} {Using} a {Soft} {Planar} {Grasping} {Manipulator}},
	volume = {2},
	issn = {2169-5172, 2169-5180},
	url = {https://www.liebertpub.com/doi/10.1089/soro.2015.0013},
	doi = {10.1089/soro.2015.0013},
	abstract = {This article presents the development of an autonomous motion planning algorithm for a soft planar grasping manipulator capable of grasp-and-place operations by encapsulation with uncertainty in the position and shape of the object. The end effector of the soft manipulator is fabricated in one piece without weakening seams using lost-wax casting instead of the commonly used multilayer lamination process. The soft manipulation system can grasp randomly positioned objects within its reachable envelope and move them to a desired location without human intervention. The autonomous planning system leverages the compliance and continuum bending of the soft grasping manipulator to achieve repeatable grasps in the presence of uncertainty. A suite of experiments is presented that demonstrates the system’s capabilities.},
	language = {en},
	number = {4},
	urldate = {2020-10-12},
	journal = {Soft Robotics},
	author = {Katzschmann, Robert K. and Marchese, Andrew D. and Rus, Daniela},
	month = dec,
	year = {2015},
	pages = {155--164},
}

@article{katzschmann_cyclic_nodate,
	title = {Cyclic {Hydraulic} {Actuation} for {Soft} {Robotic} {Devices}},
	abstract = {Undulating structures are one of the most diverse and successful forms of locomotion in nature, both on ground and in water. This paper presents a comparative study for actuation by undulation in water. We focus on actuating a 1DOF systems with several mechanisms. A hydraulic pump attached to a soft body allows for water movement between two inner cavities, ultimately leading to a ﬂexing actuation in a side-toside manner. The effectiveness of six different, self-contained designs based on centrifugal pump, ﬂexible impeller pump, external gear pump and rotating valves are compared. These hydraulic actuation systems combined with soft test bodies were then measured at a lower and higher oscillation frequency. The deﬂection characteristics of the soft body, the acoustic noise of the pump and the overall efﬁciency of the system are recorded. A brushless, centrifugal pump combined with a novel rotating valve performed at both test frequencies as the most efﬁcient pump, producing sufﬁciently large cyclic body deﬂections along with the least acoustic noise among all pumps tested. An external gear pump design produced the largest body deﬂection, but consumes an order of magnitude more power and produced high noise levels. Further reﬁnement remains on determining the suitable oscillation frequencies and inner cavity designs for optimal efﬁciency and movement.},
	language = {en},
	author = {Katzschmann, Robert},
	pages = {8},
}

@article{katzschmann_building_nodate,
	title = {Building and {Controlling} {Fluidically} {Actuated} {Soft} {Robots}: {From} {Open} {Loop} to {Model}-based {Control}},
	abstract = {This thesis describes the creation and control of soft robots made of deformable elastomer materials and powered by fluidics. We embed soft fluidic actuators into self-contained soft robotic systems, such as fish for underwater exploration or soft arms for dynamic manipulation. We present models describing the physical characteristics of these continuously deformable and fully soft robots, and then leverage these models for motion planning and closed-loop feedback control in order to realize quasi-static manipulation, dynamic arm motions, and dynamic interactions with an environment. The design and fabrication techniques for our soft robots include the development of soft actuator morphologies, soft casting techniques, and closed-circuit pneumatic and hydraulic powering methods. With a modular design approach, we combine these soft actuator morphologies into robotic systems. We create a robotic fish for underwater locomotion, as well as multi-finger hands and multi-segment arms for use in object manipulation and interaction with an environment. The robotic fish uses a soft hydraulic actuator as its deformable tail to perform open-loop controlled swimming motions through cyclic undulation. The swimming movement is achieved by a custom-made displacement pump and a custom-made buoyancy control unit, all embedded within the soft robotic fish. The fish robot receives high-level control commands via acoustic signals to move in marine environments. The control of the multi-segment arms is enabled by models describing the geometry, kinematics, impedance, and dynamics. We use the models for quasi-static closedloop control and dynamic closed-loop control. The quasi-static controllers work in combination with the kinematic models and geometric motion planners to enable the soft arms to move in confined spaces, and to autonomously perform object grasping. Leveraging the models for impedance and dynamics, we also demonstrate dynamic arm motions and end-effector interactions of the arm with an environment. Our dynamic model allows the application of control techniques developed for rigid robots to the dynamic control of soft robots. The resulting model-based closed-loop controllers enable dynamic curvature tracking as well as surface tracing in Cartesian space.},
	language = {en},
	author = {Katzschmann, Robert Kevin},
	pages = {272},
}
